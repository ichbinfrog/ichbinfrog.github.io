---
title: "üá´üá∑ Etude d'un dataset al√©atoire"
date: 2020-11-28T18:16:46+01:00
toc: true
images:
---

# Classification

## Analyse exploratoire

L‚Äôobjectif de ce travail est de proposer le meilleur classifieur possible √† partir de 500 observations, capable
de distinguer trois classes (not√© 1, 2 et 3) de proportion initiale in√©gale ($œÄ_1 = 0.18, œÄ_2 = 0.442, œÄ_3 = 0.378$).
Comme le jeu de donn√©es est synth√©tique, il est impossible d‚Äôattribuer une signification √† chaque variable.
Tout de m√™me, les variables se distinguent clairement en trois groupes:

X1 √† X20, suivant une distribution bimodale continue d‚Äôesp√©rance E\[X\] = 5 et de variance relativement
faible:
- X21 √† X45, suivant une loi normale continue d‚Äôesp√©rance E\[X\] = 0 et de variance elev√©e
- X46 √† X50, avec une distribution discr√®te positive centr√©e aux alentours de E\[X\] = 3.7

{{< image src="/img/sy19_prelim.png"  position="center" >}}

## Feature selection

Puisque nous avons une quantit√© importante de variable par rapport aux nombres d‚Äôobservations (1/10),
il serait judicieux d‚Äôappliquer des m√©thodes de r√©duction de dimensions si possible.Comme l‚Äôanalyse en
composantes principales ne fonctionne qu‚Äôavec les donn√©es quantitatives, nous posons l‚Äôhypoth√®se que les
variables X45 ‚àí X50 sont quantitatives.
Suite √† cette analyse, nous observons qu‚Äôil faut garder 40 variables afin de pr√©server 90% de la variance
expliqu√©e. Il y a donc peu d‚Äôint√©r√™t d‚Äôappliquer cette r√©duction.

## KNN

L‚Äôalgorithme des KNN consiste √† affecter √† un individu la classe dominante dans son voisinnage. En
supprimant les variables qualitatives (X46 √† X50), une LOOCV permet de montrer que le nombre de voisin
optimal est k = 15 avec un taux d‚Äôerreur de classification √† 0.572. Ce taux d‚Äôerreur √©lev√© peut s‚Äôexpliquer par
plusieurs facteurs:
- dans l‚Äôespace de d√©part les centres d‚Äôinertie des classes sont proches les unes des autres
- la proportion in√©gale des classes pousse le classifieur √† toujours choisir la classe 2
- la distance euclidienne utilis√©e par d√©faut est tr√®s sensible √† un nombre √©lev√© de pr√©dicteurs

Les techniques d‚Äôanalyses discriminantes vues en cours peuvent √™tre facilement √©tendues √† des probl√®mes
de classification multi-classe car l‚Äôappartenance √† une classe revient √† trouver la probabilit√© √† posteriori
maximale pour l‚Äôindividu donn√©.

## Analyse discriminante

Puisque les fronti√®res construites par la LDA sont lin√©aires et que dans l‚Äôespace de d√©part, il n‚Äôy a pas
d‚Äôhyperplans discriminants apparents, la qualit√© du classifieur qui en r√©sulte est m√©diocre avec un taux
d‚Äôerreur de classification de 0.42.

De plus, un test de Barlett sur le jeu de donn√©es permet de rejetter l‚Äôhypoth√®se nulle d‚Äôhomosc√©dasticit√©
entre les diff√©rentes classes √† un niveau de confiance √† $2.2e^{‚àí16}$. Nous pouvons donc tenter d‚Äô√©liminer cette
hypoth√®se en utilisant l‚Äôanalyse discriminante quadratique (QDA). Avec une validation crois√©e √† 10 plis, nous
obtenons les taux d‚Äôerreur de classification suivant:

{{< image src="/img/sy19_lda.png"  position="center" >}}


La variance √©lev√©e observ√©e d‚Äôun mod√®le QDA par rapport √† un mod√®le LDA peut s‚Äôexpliquer par le fait qu‚Äôil
doit estimer plus de param√®tres sur un √©chantillon de taille relativement faible. De plus, la QDA fait moins
d‚Äôerreur de classification pour les individus de classe 3 et l√©g√®rement plus d‚Äôerreur pour les individus de classe
1, ce qui donne en g√©n√©rale un meilleur mod√®le sachant que $œÄ_3 > œÄ_1$.

Les mod√®les √©tudi√©es ci-dessus doivent estimer un nombre de param√®tres proportionnel aux nombres de
variables et de classes. Or, dans notre cas, avec 50 variables et 3 classes, la QDA doit estimer 3977 param√®tres
et la LDA 1427, ce qui va faire exploser la variance des mod√®les obtenus.

Pour palier √† ce probl√®me, en supposant l‚Äôind√©pendance des pr√©dicteurs conditionnellement √† leurs classes, on
peut construire un classifieur bay√©sien na√Øf avec moins de param√®tres √† estimer et donc plus adapt√© √† notre
nombre de variables. Malgr√© sa simplicit√©, le bay√©sien na√Øf offre une meilleure performance g√©n√©rale que la
LDA et un meilleure classification de la classe 1 que la QDA.

### R√©gularisation

L‚Äôanalyse discriminante r√©gularis√©e (RDA) permet de trouver un compromis entre le biais √©lev√© des LDA et
la variance √©lev√©e des QDA. Au lieu d‚Äôestimer une matrice de variance commune (\hat{\Sigma}) comme dans la LDA ou
des matrices de variance ($\hat{\Sigma}_k$) pour chaque classe, elle construit une matrice de covariance r√©gularis√©e de la
forme: $\hat{\Sigma}_k(\lambda) = (1 ‚àí \lambda)\hat{\Sigma}_k + \lambda\hat{\Sigma}$. Lorsque $\lambda \rightarrow 0$, le classifieur se rapproche d‚Äôune QDA et lorsque $\lambda \rightarrow 1$, le
mod√®le tends vers une LDA.

{{< image src="/img/sy19_rda.png"  position="center" >}}

En faisant une recherche syst√®matique dans l‚Äôespace des \lambda, on peut conclure qu‚Äôun coefficient de r√©gularisation
$\lambda \in [0.50, 0.55]$ donne le mod√®le optimal qui favorise l√©g√®rement l‚Äôhomosc√©dasticit√© des variances.
L‚Äôanalyse RDA admet aussi un coefficient $\gamma$ qui permet de r√©gulariser $\hat{\Sigma}$ selon la formule $(1 ‚àí \gamma)\hat{\Sigma} + \gamma\sigma^2
I_p$.

La fonction `rda` dans la librairie `klaR` qui permet d‚Äôoptimiser les deux param√®tres n‚Äôest pas stable pour les
jeu de donn√©es de petite taille. Nous pouvons lancer plusieurs it√©rations et identifier les zones optimales
o√π se trouvent le plus fr√©quemment les solutions et o√π le taux d‚Äôerreur de classification est le plus faible.
Nous trouvons donc que les couples de param√®tres optimales sont comprises entre $\lambda \in [0.04, 0.05]$ (ce qui
correspond √† une QDA) et $\gamma \in [0.50, 0.055]$.

{{< image src="/img/sy19_qda.png"  position="center" >}}


### Stepwise selection

La fonction `stepclass` de la librairie `klaR` adapte la stepwise selection √† la classification et donne directement
le sous-ensemble de pr√©dicteur optimal qui garantit le taux d‚Äôerreur de classification issu d‚Äôune validation
crois√©e le plus faible. Nous observons donc que cette s√©lection des variables √©change la performance moyenne
du classifieur pour r√©duire la variance du mod√®le

## R√©gression Logistique

Contrairement √† l‚Äôanalyse discriminante lin√©aire, la r√©gression logistique est un mod√®le plus robuste qui ne
pr√©sume pas que les pr√©dicteurs suivent une distribution gaussienne multidimensionnelle et fonctionne avec
les variables qualitatives. Tout de m√™me, en appliquant une r√©gression multinomiale sur l‚Äôensemble du jeu de
donn√©es, le taux moyen d‚Äôerreur de classification issue d‚Äôune validation crois√©e √† 10-plis est aux alentours de
0.45 et d√©passe celui obtenu avec une LDA.

Pour am√©liorer cette performance, il serait judicieux d‚Äô√©liminer les pr√©dicteurs ‚Äúredondants‚Äù qui sont trop
fortement correl√©s entre eux puis les pr√©dicteurs √† effet n√©gligeable sur le mod√®le. La matrice de correlation
des donn√©es, elle, r√©v√®le aucune correlation forte entre les pr√©dicteurs donc il faut remettre en question la
significativit√© des variables. Les m√©thodes de r√©gularisation qui suppriment les coefficients n√©gligeables tel
que le lasso ou qui les p√©nalisent tel que l‚Äôelastic net peuvent r√©soudre ce probl√®me

{{< image src="/img/sy19_logreg.png"  position="center" >}}

Avant d‚Äôappliquer la r√©gularisation, les variables quantitatives sont centr√©es et r√©duites afin d‚Äôaugmenter la
stabilit√© des estimations des coefficients et donc du mod√®le puis rattach√©es aux variables qualitatives. Nous
observons alors que la r√©gularisation r√©duit de mani√®re non n√©gligeable le taux d‚Äôerreur de classification de la
r√©gression logistique.

En prenant la valeur de lambda la plus r√©gularis√©e selon la r√®gle du 1-se, les coefficients $\beta$ obtenus entre le
lasso et l‚Äôelasticnet sont proches et r√©v√®lent l‚Äôimportance des variables qualitatives dans la classification des
individus 1 et 2.

{{< image src="/img/sy19_lasso.png"  position="center" >}}


## Arbres et for√™ts al√©atoires

En r√©gularisant l‚Äôarbre de d√©cision par post-√©lagage, nous obtenons une erreur de validation crois√©e optimale
pour un arbre de profondeur 5 de 0.46. Avec la for√™t al√©atoire, nous obtenons un mod√®le de variance plus
faible mais fortement biais√© (la for√™t d√©cide rarement pour la classe 1) avec une erreur OOB de 0.394. M√™me
apr√®s avoir augment√© le poids de cette classe dans l‚Äôapprentissage, ce probl√®me n‚Äôest pas r√©solu.

## Conclusion

| Mod√®le | Taux d‚Äôerreur de classification |
|:---:|:---:|
| KNN | 0.572 |
| LDA | 0.616 |
| LDA subset | 0.375 |
| QDA | 0.321 |
| QDA subset | 0.314 |
| RDA | 0.303 |
| RDA subset | 0.303 |
| Logit | 0.450 |
| Logit Ridge | 0.386 |
| Logit Elasticnet | 0.376 |
| Logit Lasso | 0.376 |
| Random Forest | 0.394 |

Le mod√®le RDA est donc optimal pour classifier ce jeu de donn√©es. Dans nos futurs travaux, il serait
int√©ressant d‚Äôadresser le d√©s√©quilibre des classes (avec du under-sampling ou du SMOTE) et de tester d‚Äôautre
m√©thodes supervis√©es (SVM ou SGD).

---

# R√©gression

L‚Äôobjectif de ce travail est de proposer la meilleure r√©gression sur la variable y (E[y] = 161.65) √† partir d‚Äôun
√©chantillon de 500 r√©alisations de 100 variables. Similairement au probl√®me de classification, aucune une
signification peut √™tre attribuer √† chaque variable. Tout de m√™me, contrairement aux donn√©es de classification,
l‚Äôensemble des variables sont quantitatives continues d‚Äôesp√©rance communes proches de E[X] = 5 qui est
constitu√© de deux groupes:

- un groupe majoritaire suivant une distribution bimodale continue
- une minorit√© (dont notamment X50, X65) qui suit peut-√™tre une distribution multimodale (k = 3)

Quant aux r√©alisations de la variable √† pr√©dire, elle, suit clairement une loi normale avec $\hat{\mu} = 161.65$ et
$\hat{\sigma} = 82.67529$.

{{< image src="/img/sy19_prelim_reg.png"  position="center" >}}

## Feature selection

Pour ce jeu de donn√©es, le probl√®me du rapport entre le nombre de pr√©dicteurs et le nombre d‚Äôindivdus est
encore plus prononc√© (1/5). D‚Äôo√π la n√©cessit√© de r√©duire si possible la dimension par PCA. Tout de m√™me,
afin de maintenir 90 de la variance expliqu√©e cumul√©e, 80 axes doivent √™tre maintenus et la m√©thode du coude
ne r√©v√®le pas non plus un nombre de composantes principales optimal.

{{< image src="/img/sy19_pca.png"  position="center" >}}

## KNN

L‚Äôalgorithme des knn se fonde sur un vote majoritaire des individus dans le nuage. Cette proximit√© se d√©finit
par une distance (euclidienne par d√©faut dans l‚Äôimpl√©mentation dans la librairie `MASS`) commun par rapport √†

tous les dimensions. Or, dans un jeu de donn√©es non standardis√©s, les √©chelles entre les dimensions peuvent
diff√©rer. D‚Äôo√π, la n√©cessit√© de normaliser (centrage et r√©duction) au pr√©alable les donn√©es.
Avec ce pr√©traitement sur l‚Äôensemble des pr√©dicteurs (y exclu), l‚Äôerreur quadratique moyenne du mod√®le avec
le nombre de voisins optimal obtenue par validation crois√©e imbriqu√©e am√©liore de mani√®re n√©gligeable de
5099.594 √† 5082.458. Malgr√© cette performance m√©diocre, ceci confirme l‚Äôobservation que les donn√©es initiales
sont relativement ‚Äúhomog√®nes‚Äù (moyenne et √©cart-type similaire).

## Mod√®les lin√©aires

De premi√®re vue, il est tr√®s probable que y soit une combinaison lin√©aire des diff√©rents pr√©dicteurs et donc
qu‚Äôun mod√®le lin√©aire sera optimal pour notre probl√©matique.

Dans un premier temps, nous avons estim√© le vecteur des coefficients $\hat{\beta}$ de la r√©gression par la m√©thode des
moindres carr√©s sur 75 des observations. Le mod√®le qui en r√©sulte est bien significatif car nous obtenons une
pvalue < $2.2e^{‚àí16}$ pour l‚Äôhypoth√®se de nullit√© de l‚Äôensemble des param√®tres.

{{< image src="/img/sy19_qqplot.png"  position="center" >}}

De plus, nous constatons que les r√©sidus ne semble pas suivre une structure particuli√®re et que les r√©sidus
standardis√©s √©pousent bien le qqplot d‚Äôune loi normale. Ceci confirme l‚Äôhypoth√®se de normalit√© des r√©sidus et
nous pouvons donc tenter de faire une validation crois√©e pour estimer la performance du LSE sur nos donn√©es.
Nous obtenons donc une bonne erreur quadratique moyenne de 260.1426 pour un mod√®le relativement simple.

### Subset selection

A partir du mod√®le pr√©c√©dent, une question peut se poser: est-ce qu‚Äôil y a certaines variables non significatives
ou qui nuisent √† la r√©gression? En √©tudiant l‚Äô√©volution du coefficient de corr√©lation $R^2$ en fonction du nombre 
de pr√©dicteurs pris en compte, la m√©thode du subset selection permet de montrer que pour maintenir un
mod√®le de performance similaire, il suffit de maintenir 80 variables.

{{< image src="/img/sy19_mse.png"  position="center" >}}

L‚Äôusage de cette m√©thode permet de r√©duire la variance du mod√®le obtenu ainsi que la moyenne des erreurs
quadratiques observ√©es √† 235.1.

### R√©gularisation

Similairement √† la r√©gression logistique dans la classification, il est possible de r√©gulariser la r√©gression lin√©aire
en p√©nalisant les coefficients par la m√©thode Ridge ou le Lasso. En faisant une validation crois√©e imbriqu√©e
pour optimiser les valeurs de $\lambda$, nous obtenons cette comparaison de la qualit√© des diff√©rentes m√©thodes de
r√©gularisation.

{{< image src="/img/sy19_ridge.png"  position="center" >}}

Nous observons donc que parmi les r√©gularisations, le Ridge et le mod√®le de compromis (Elastic Net) d√©passent
clairement le lasso en terme de performance. Ceci sugg√®re que le nombre de variables important dans la
pr√©diction de y est important et la suppression de ces variables par lasso n‚Äôest pas n√©cessaire. De plus, nous
observons que la standardisation (centrage et r√©duction) des donn√©es activ√©e par d√©faut dans la fonction
glmnet affaiblit le lasso puisqu‚Äôelle acc√©l√®re peut-√™tre trop fortement la contraction des coefficients.

### Splines et GAM

Il est possible que la variable √† pr√©dire r√©sulte d‚Äôune combinaison non lin√©aire des pr√©dicteurs. Comme nous
avons un jeu de donn√©es de dimension > 2, deux mod√®les multidimensionnels ont √©t√© abord√©s en cours:
- Les splines multidimensionnels qui combinent les diff√©rentes fonctions de transformation par produit.
Avec 100 pr√©dicteurs, il est impossible d‚Äôappliquer ce mod√®le sans s√©lection pr√©alable √† cause du nombre
de param√®tres √† estimer.
- Les mod√®les additifs g√©n√©ralis√©s (GAM) qui cherchent 100 fonctions transformant chacun une variable
au lieu d‚Äôune fonction √† 100 dimensions.

En appliquant le gam avec un type de spline unique sur l‚Äôensemble des pr√©dicteurs sans standardisation, nous
obtenons les erreur quadratique suivante √† l‚Äôissu d‚Äôune validation crois√©e √† 10 plis:

{{< image src="/img/sy19_gam.png"  position="center" >}}

La forme des b-splines cubiques (`gam bs(df=3)`) et des splines naturels (`gam ns(df=3)`) sont tr√®s similaires
mis √† part au niveau des fronti√®res de l‚Äôespace de y. Comme le gam est additif, ces similarit√©s s‚Äôaggr√®gent
dans le mod√®le final et explique la proximit√© entre les erreurs.

{{< image src="/img/sy19_spline.png"  position="center" >}}

Quant aux smoothing splines avec un degr√© de libert√© de 2, nous obtenons un mod√®le de performance
relativement proche au subset. En examinant de pr√®s ce mod√®le, nous observons que seulement 68 variables
sont significatives (√† l‚Äôissu d‚Äôune analyse de variance param√©trique) et qu‚Äôil n‚Äôy aucune transformation de la
pr√©diction (fonction de lien: `identity`).

## Conclusion

| Mod√®le  | MSE moyenne |
|:---:|:---:|
| KNN  | 5082.458 |
| Linear Regression (LR) | 260.1426 |
| LR subset selection  | 235.1 |
| Ridge regression | 259.8 |
| Elasticnet regularisation | 249.9 |
| Lasso regularisation (without standardisation)  | 317.5 |
| GAM (b-spline,df=3)  | 429.3 |
| GAM (natural spline, df=2)  | 294.8 |
| GAM (smoothing spline, df=2)  | 233.2 |

Le GAM constitu√© uniquement de smoothing spline sur l‚Äôensemble des pr√©dicteurs non standardis√©s est donc
optimal pour notre jeu de donn√©es car il donne l‚Äôerreur quadratique moyenne minimal ainsi qu‚Äôune variance
relativement faible.

Dans nos futurs travaux, il serait int√©ressant d‚Äôeffectuer une s√©lection pr√©alable des variables avant le
GAM et tester d‚Äôautre mod√®les comme les machines √† vecteurs de support (SVM) ou m√™me les splines
multidimensionnels (apr√®s une r√©duction de dimension par manifold learning).