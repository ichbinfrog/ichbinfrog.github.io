---
title: "üá´üá∑ METAR data analysis & warehousing with Cassandra"
date: 2018-12-21T15:37:55+02:00
draft: false
toc: true
images:
tags:
  - datawarehouse
  - cassandra
---

Dans le cadre de ce projet, nous avons construit un stockage orient√© colonne destin√© √† r√©pondre √† un ensemble de question sur des donn√©es METAR (c'est-√†-dire des donn√©es m√©t√©orologiques pour l'aviation). D'une mani√®re plus concr√®te, notre jeu de donn√©es est restreint dans l'espace (exclusivement en Finlande) et dans le temps (entre le 1er Janvier 2004 au 1er Janvier 2014). Puisque dans ce cas, les trois questions ne peuvent pas √™tre r√©pondus par une mod√©lisation commune, nous allons traiter dans ce rapport selon l'ordre des questions.


## Pr√©visualisation du jeu de donn√©es
Les donn√©es venant de la base de donn√©es IEM (Iowa Environmental Mesonet) correspondent √† des donn√©e ASOS (une sous-famille des donn√©es METAR) sont constitu√©es de $30$ variables. 

Les attributs $\{timestamp, longitude, latitude, altitude\}$ permettent de d√©finir d'une mani√®re unique un enregistrement, autrement dit, √† un instant $t$ donn√©, √† la latitude $lat$, la longitude $long$ et l'altitude $alt$, il ne peut exister qu'une seule observation. L'unicit√© de l'observation est renforc√© par le formattage du timestamp qui est normalis√© sur UTC (Universel Temps Coordonn√©). Dans les sections qui suivent, il est important de noter que toutes les donn√©es nulles/absentes sont not√©es "null" et que les donn√©es forment des vecteurs creuses.

## Premi√®re question
> Pour un point donn√© de l‚Äôespace, je veux pouvoir avoir un historique du pass√©, avec des courbes adapt√©es. Je vous pouvoir mettre en √©vidence la saisonnalit√© et les √©carts √† la saisonnalit√©.

{{< image src="/img/metar_geohash.jpg" position="center" >}}

Afin de pouvoir identifier un **point donn√© de l'espace**, nous pouvons utilis√© le tripplet $(long, lat, alt)$. N√©anmoins, cette m√©thode est tr√®s restrictive notamment si on veut appliquer cette requ√™te sur une zone dans l'espace. Nous avons donc choisi d'utiliser le **geohash**. Ce dernier est une fonction de hashage qui subdivise la surface terrestre selon une grille hi√©rarchique. D'une mani√®re plus concr√®te, soit $G \subset \mathbb{R}^{2}$ l'ensemble des coordonn√©es g√©ographiques et $W_p$ l'ensemble des cha√Ænes alphab√©tiques de longueur $p$ (qui correspond √† la pr√©cision du geohash), on a:

$$geohash :\quad G \rightarrow W_p$$

$$(lat,lon) \mapsto geohash(lat, lon)\ avec\ len(geohash)=p$$

N√©anmoins, mettre l'ensemble du geohash en cl√© de partitionnement va nous aboutir au m√™me probl√®me d'avoir des partitions trop fines. Puisqu'il s'agit d'un hash hi√©rarchique, le geohash permet de s'assurer que deux hash commen√ßant par la m√™me lettre sont dans le m√™me pav√©, ce qui garantit la coh√©rence des donn√©es vis-√†-vis du partitionnement. On peut donc s√©parer le hash en deux, une partie allant dans la cl√© de partitionnement (qu'on note $geohash_{key}$), l'autre dans la surclef ($geohash_{part}$). Gr√¢ce √† l'algorithme de preuve (Voir helper.py), on a d√©duit qu'avec notre jeu de donn√©es, la conception optimale sera : 

$$geohash :\quad G \rightarrow W_p$$

$$\quad (lat,lon) \mapsto geohash(lat, lon) = geohash_{key} + geohash_{hash}$$

$$avec\ len(geohash_{key})=2\ et\ len(geohash_{part})=p -2$$

En ce qui concerne l'**historique**, nous avons choisi de clusteriser selon l'ann√©e afin de faciliter la lecture tout en permettant √† un partitionnement capable d'√©voluer au cours du temps. De plus, √©tant donn√© qu'il s'agit d'un historique (c'est-√†-dire qu'on cherche toutes les informations depuis une date pr√©cise jusqu'√† la date actuelle), on peut impl√©menter des cl√©s de tris du type $mois, jour, heure, seconde$. N√©anmoins, cette impl√©mentation pose probl√®me avec l'historique puisque ce dernier n√©cessite une suite d'op√©rateurs de comparaison $>$ (interdite par scylla). On a donc concat√©n√© ces valeurs dans un entier $timestamp_{rest}$ qui permettrait cette comparaison. 

De plus, afin de pouvoir construire des courbes d'√©volution capable de d√©montrer la **saisonnalit√©**, nous avons impl√©ment√© des courbes d'√©volution des variables quantitatives (par exemple la temp√©rature ressentie). Finalement, nous avons la conception qui suit;

{{< image src="/img/metar_conception_1.png" position="center" >}}



## Deuxi√®me question
> √Ä un instant donn√© je veux pouvoir obtenir une carte me repr√©sentant n‚Äôimporte quel indicateur.

Contrairement √† la conception pr√©c√©dente, dans cette requ√™te, l'hi√©rarchie temporelle emporte par rapport √† l'hi√©rarchie g√©ographique. De plus, contrairement √† l'historique, la requ√™te porte sur un instant sp√©cifique et non pas une p√©riode. 

En terme de cl√© de partition, nous avons choisi d'utiliser un couple $(ann√©e, mois)$ afin de constituer un nombre raisonnable de partitions th√©oriquement homog√®nes (Avec le jeu de donn√©es en question, on aurait $nb_{annee} \times nb_{mois} = 10 \times 12 = 120$ partitions contenant potentiellement de $\underbrace{545791}_\textrm{nb enregis.} / 120 = 4500 $ enregistrements par partition). Pour composer la surclef, on ajoute les attributs heure, minute, seconde (afin de compl√©ter l'hi√©rarchie temporelle), la longitude, la latitude et l'altitude. L'ordre des clefs de tri permet de requ√™ter la base sans conna√Ætre la position exacte de l'observation. Finalement, ceci aboutit √† la mod√©lisation qui suit:

{{< image src="/img/metar_conception_2.png" position="center" >}}

En terme de repr√©sentation, nous avons choisi de superposer nos latitudes et nos longitudes sur une carte interactive de la Finlande construite par le module folium. De plus, puisque la question n'impose aucune contrainte d'interpr√©tation, les donn√©es en question sont interpr√©t√©es sous l'angle d'analyse des temp√©ratures ressenties, c'est-√†-dire que pour un instant donn√©, on veut savoir quelle est la plage de temp√©rature mesur√©e √† une station donn√©e. Sur la carte, les temp√©ratures seront repr√©sent√©es avec des couleurs diff√©rentes et les autres indicateurs dans le popup correspondant. 



## Troisi√®me Question

> Pour une p√©riode de temps donneÃÅe, je veux pouvoir clusteriser l‚Äôespace, et repr√©senter cette clusterisation.

Similairement √† la question 2, l'hi√©rarchie temporelle remporte sur l'hi√©rarchie g√©ographique. Afin de faciliter l'interrogation des donn√©es requises, nous avons concat√©n√© les attributs ann√©e, mois et jour (afin de constituer un entier sous format YYYYmmdd) pour former une cl√© de partition. A ceci, s'ajoute les clefs de tri heure et minute qui facilite le requ√™tage par rapport √† une p√©riode sp√©cifique d'une journ√©e. Ceci a donc aboutit √† la conception suivante:

{{< image src="/img/metar_conception_3.png" position="center" >}}


Afin de clusteriser les donn√©es provenant de diff√©rentes stations, nous avons choisi d'utiliser l'algorithme des k-means, ce dernier prenant en param√®tre un ensemble de colonnes du jeu de donn√©e et en hyper-param√®tre une valeur k enti√®re. Nous avons choisi de l'impl√©menter sur qu'une seule dimension d'une part par d√©faut de temps, d'autre part √† cause du fait que chaque dimension est ind√©pendante.

L'algorithme pr√©c√©dente appliqu√©e sur la temp√©rature ambiante entre 01/06/2010 et 01/06/2011 permet de construire le diagramme de Vorono√Ø suivant:

{{< image src="/img/metar_voronoi.png" position="center" >}}

Nous avons choisi de clusteriser avec $k=3$ et $k=4$ pour comparer les r√©sultats. Pour cas $k=3$, on peut distinguer trois groupes qui sont situ√©s au sud, au centre et au nord. C'est logique comme on utilise le temp√©rature comme le pr√©dicteur, et il d√©pend fortement de latitude quand le longitude est fix√©. Et pour cas $k=4$, on peut voir il a s√©par√© un des trois groupes pr√©c√©dant, et le critique g√©n√©ral pour distinguer les clusters est encore le latitude. Pour avoir une vue plus intuitive des clusters, nous les avons projet√©s sur une carte, les points avec les m√™mes couleurs repr√©sentent un cluster.

{{< image src="/img/metar_folium.png" position="center" >}}


## Analyse des performances

Afin d'√©tudier l'efficacit√© de notre conception, nous avons effectu√© une benchmark avec l'ensemble du jeu de donn√©e charg√© sur les trois serveurs qui nous sont attribu√©s. Les indicateurs pr√©sent√©s sont obtenus gr√¢ce √† la commande: 
```sh
nodetool tablestats keyspace_name
```

|         SCTS Percentile         |   Q1   |   Q2   |   Q3   |
|:-------------------------------:|:------:|:------:|:------:|
|          SSTable Count          |    1   |    3   |    2   |
|            Space used           | 7.67e6 | 7.51e6 | 3.55e6 |
|   SSTable  Compression Ration   |  0.318 |  0.377 |  0.451 |
|  Number of estimated partitions |   76   |  2317  |  2418  |
| Compacted partition  mean bytes |   1e6  |  1.3e4 |   5e3  |

---
|         DTCS Percentile        |   Q1   |   Q2   |   Q3   |
|:------------------------------:|:------:|:------:|:------:|
|          SSTable Count         |    2   |    2   |    1   |
|           Space used           | 3.36e7 | 3.61e7 | 6.45e5 |
|   SSTable Compression Ration   |  0.317 |  0.377 |  0.496 |
| Number of estimated partitions |   98   |  2379  |  2483  |
| Compacted partition mean bytes | 1.21e6 | 4.35e4 |   5e3  |

---
|         TWCS Percentile        |   Q1   |   Q2   |   Q3   |
|:------------------------------:|:------:|:------:|:------:|
|          SSTable Count         |    1   |    3   |    2   |
|           Space used           | 3.38e7 | 3.61e7 | 5.37e6 |
|   SSTable Compression Ration   |  0.32  |  0.378 |  0.48  |
| Number of estimated partitions |   98   |  2379  |  2484  |
| Compacted partition mean bytes | 6.78e5 |  4.3e4 |   5e3  |

Le crit√®re du **nombre de SSTable** correspond au nombres magasins cl√© valeur n√©cessaires pour indexer les contenus de la table en question. En r√©alit√©, √† chaque √©criture, Cassandra √©crit la donn√©e dans une table memtable et un commit log, ces derniers √©tant mis en m√©moire physique (dans une SSTable) une fois un threshold d√©pass√©. Donc plus le nombre de SSTable est important, plus la possibilit√© qu'une donn√©e cherch√©e soit r√©partie sur plusieurs SSTable, ce qui augmente la latence des lectures r√©duisant ainsi la performance. Nous observons donc que la latence d'√©criture ne fera pas l'objet de notre optimisation.

Le deuxi√®me crit√®re de performance s'agit du **ratio de compaction des SSTables** correspond √† un indicateur sur l'efficacit√© du stockage en m√©moire. La compaction correspond √† un rassemblement  On observe donc un goulot d'√©tranglement du syst√®me au niveau de la m√©moire puisque ce facteur est relativement faible (environ 0.3 - 0.4). Malgr√© le fait que nos donn√©es sont "sparse", la strat√©gie de compaction par d√©faut SizeTieredCompactionStrategy (ne s'applique que lorsqu'un threshold du nombre de SSTable est d√©pass√©) n'est donc pas adapt√©e. Puisque nos donn√©es pr√©sentent √† chaque fois une dimension temporelle, les strat√©gies DateTieredCompactionStrategy (DTCS) et TimeWindowCompactionStrategy (TWCS) sont donc plus adapt√©es (notamment avec la question 3).

Mis √† part ces options, notre mod√©lisation peut aussi √™tre remise en question gr√¢ce aux donn√©es sur le **nombre de partitions estim√©** ainsi que la **distribution des donn√©es par partition** avec la commande : 
```sh
nodetool cfhistograms keyspace_name.table_name
```

| Percentile | Q1 Cell Count | Q2 Cell Count | Q3 Cell Count |
|:----------:|:-------------:|:-------------:|:-------------:|
|     .5     |     182785    |      446      |      642      |
|     .75    |     182785    |      446      |      770      |
|     .95    |     263210    |      9887     |      924      |
|     .98    |     315852    |     11864     |      1109     |
|     .99    |     315852    |     14237     |      1109     |
|     min    |     35426     |      104      |       2       |
|     max    |     315852    |     17084     |      1109     |

Il est important de contextualiser le fait que notre jeu de donn√©e est restreint g√©ographiquement et la mod√©lisation propos√©e est destin√©e √† √™tre scalable √† ce niveau. En terme de distribution, la distribution est relativement homog√®ne puisque les quartiles sont proches. Le nombre de partitions faibles observ√©es √† la question 1 et sa distribution confirme donc la validit√© de notre conception. 

De m√™me, les mod√©lisations r√©pondant aux autres questions engendrent la cr√©ation de 1000 fois plus de partition. Cette diff√©rence, malgr√© √©norme peut √™tre expliqu√© gr√¢ce √† l'hi√©rarchie temporelle construite aboutissant √† environ 13 partitions par mois. Contrairement √† la premi√®re conception, celles-ci pr√©sentent une distribution fortement h√©t√©rog√®ne qui peut, en partie, √™tre expliqu√©e par la distribution des dates des mesures m√©t√©orologiques. Tout de m√™me, il faudrait remettre en question ces mod√©lisations.


## Conclusion

En conclusion, √† travers ce projet, nous avons propos√© un ensemble de mod√©lisations adaptables avec une interface permettant de les exploiter. Nous avons √©t√© confront√© √† des probl√©matiques de gestion et d'exploitation des donn√©es r√©elles, imparfaites, mal format√©es, h√©t√©rog√®nes et parfois inexploitables. 

N√©anmoins, il reste beaucoup de points √† travailler pour que ce dernier peut r√©pondre √† des usages en environnement de production. En terme d'am√©liorations possibles, nous pouvons optimiser les calculs (moyenne, m√©diane, etc...) et impl√©menter des algorithmes de Statistical Learning plus approfondie en le distribuant avec Spark. De m√™me par d√©faut de temps, nous n'avons pas pu construire des outils de visualisation plus intuitive pour exploiter les donn√©es, ni optimiser le d√©ploiement avec des param√®tres plus avanc√©es (comme par exemple la classe de Compression, la gestion des Tombstones, etc...).     