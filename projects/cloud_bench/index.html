<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="">
<meta name="description" content="This benchmark is done in order to help in the process of deciding which public provider to migrate to. Three public cloud offers are studied (GCP (Google Cloud Platform), AWS (Amazon Web Service) and Microsoft Azure) in comparison to the private solution.
Computing offer comparison It is important to note that all the aforementioned providers have support for Linux as well as Windows VMs. Although terminologies vary widely between providers (machine type for GCP, instance for AWS, VM for Azure), they all provide a couple of essential categories of common machine families." />
<meta name="keywords" content=", PerfKitBenchmarker, GCP, AWS, Azure, Database, Networking, Automation" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="https://ichbinfrog.github.io/projects/cloud_bench/" />
<script type="text/javascript" async
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$']],
            processEscapes: true,
            processEnvironments: true,
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            TeX: {
                equationNumbers: { autoNumber: "AMS" },
                extensions: ["AMSmath.js", "AMSsymbols.js"]
            }
        }
    });
    MathJax.Hub.Queue(function () {
        
        
        
        var all = MathJax.Hub.getAllJax(), i;
        for (i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });

    MathJax.Hub.Config({
        
        TeX: { equationNumbers: { autoNumber: "AMS" } }
    });
</script>


    <title>
        
            Cloud Provider benchmark :: Hoang Quoc Trung 
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="/main.min.cbd9ce6c3a26d6c4a5eb4b8834c18d83518654bd8bac1fc8160486f336181c9a.css">


    
        <link rel="stylesheet" type="text/css" href="/css/post.css">
    





<meta itemprop="name" content="Cloud Provider benchmark">
<meta itemprop="description" content="This benchmark is done in order to help in the process of deciding which public provider to migrate to. Three public cloud offers are studied (GCP (Google Cloud Platform), AWS (Amazon Web Service) and Microsoft Azure) in comparison to the private solution.
Computing offer comparison It is important to note that all the aforementioned providers have support for Linux as well as Windows VMs. Although terminologies vary widely between providers (machine type for GCP, instance for AWS, VM for Azure), they all provide a couple of essential categories of common machine families.">
<meta itemprop="datePublished" content="2019-12-15T12:50:00&#43;00:00" />
<meta itemprop="dateModified" content="2019-12-15T12:50:00&#43;00:00" />
<meta itemprop="wordCount" content="5750">
<meta itemprop="image" content="https://ichbinfrog.github.io/"/>



<meta itemprop="keywords" content="PerfKitBenchmarker,GCP,AWS,Azure,Database,Networking,Automation," />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://ichbinfrog.github.io/"/>

<meta name="twitter:title" content="Cloud Provider benchmark"/>
<meta name="twitter:description" content="This benchmark is done in order to help in the process of deciding which public provider to migrate to. Three public cloud offers are studied (GCP (Google Cloud Platform), AWS (Amazon Web Service) and Microsoft Azure) in comparison to the private solution.
Computing offer comparison It is important to note that all the aforementioned providers have support for Linux as well as Windows VMs. Although terminologies vary widely between providers (machine type for GCP, instance for AWS, VM for Azure), they all provide a couple of essential categories of common machine families."/>



    <meta property="og:title" content="Cloud Provider benchmark" />
<meta property="og:description" content="This benchmark is done in order to help in the process of deciding which public provider to migrate to. Three public cloud offers are studied (GCP (Google Cloud Platform), AWS (Amazon Web Service) and Microsoft Azure) in comparison to the private solution.
Computing offer comparison It is important to note that all the aforementioned providers have support for Linux as well as Windows VMs. Although terminologies vary widely between providers (machine type for GCP, instance for AWS, VM for Azure), they all provide a couple of essential categories of common machine families." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ichbinfrog.github.io/projects/cloud_bench/" />
<meta property="og:image" content="https://ichbinfrog.github.io/"/>
<meta property="article:published_time" content="2019-12-15T12:50:00+00:00" />
<meta property="article:modified_time" content="2019-12-15T12:50:00+00:00" /><meta property="og:site_name" content="Hoang Quoc Trung" />






    <meta property="article:published_time" content="2019-12-15 12:50:00 &#43;0000 UTC" />








    </head>

    <body class="">
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text">ichbinfrog</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="https://ichbinfrog.github.io/about">About</a></li><li><a href="https://ichbinfrog.github.io/projects">Projects</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            
            <span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>
</span>
        </span>
    </span>
</header>


            <div class="content">
                
    <main class="post">

        <div class="post-info">
            
            </p>
        </div>

        <article>
            <h2 class="post-title" id="top"><a href="https://ichbinfrog.github.io/projects/cloud_bench/">Cloud Provider benchmark</a></h2>
            <aside id="toc"><style>
.toc {
    position: fixed;
    top: 50%;
    left: 2%;
    width: 20%;
    transform: translateY(-50%);
    border-radius: 5px;
    padding-bottom: 1rem;
}

.toc a {
    text-decoration: none;
    color: gray;
}

.toc a:hover {
    opacity: 0.5;
}

.toc ul {
    list-style: ">  ";
}

.toc-title {
    text-decoration: underline;
}
</style>

<div class="toc">
    <div class="toc-title">Table of Contents</div>
    <nav id="TableOfContents">
  <ul>
    <li><a href="#computing-offer-comparison">Computing offer comparison</a></li>
    <li><a href="#hosted-database-offers">Hosted database offers</a>
      <ul>
        <li><a href="#cost">Cost</a></li>
        <li><a href="#services">Services</a></li>
        <li><a href="#performance">Performance</a></li>
      </ul>
    </li>
    <li><a href="#performance-benchmark">Performance benchmark</a>
      <ul>
        <li><a href="#reproduction-environment">Reproduction environment</a></li>
        <li><a href="#computing-instance">Computing instance</a></li>
      </ul>
    </li>
    <li><a href="#databases-and-filesystem-performance">Databases and filesystem performance</a>
      <ul>
        <li><a href="#fio--io-speed">Fio — I/O speed</a></li>
        <li><a href="#redis--key-value-data-stores">Redis — key value data stores</a></li>
        <li><a href="#mongodb--document-oriented-databases">MongoDB — document oriented databases</a></li>
        <li><a href="#cassandra--column-oriented-databases">Cassandra — column oriented databases</a></li>
      </ul>
    </li>
    <li><a href="#networking">Networking</a>
      <ul>
        <li><a href="#instance-throughput">Instance throughput</a></li>
        <li><a href="#webserver-latency">Webserver latency</a></li>
        <li><a href="#k8s-cluster-dns-resolution">k8s Cluster DNS resolution</a></li>
      </ul>
    </li>
    <li><a href="#ease-of-automation-and-tooling">Ease of Automation and Tooling</a>
      <ul>
        <li><a href="#azure">Azure</a></li>
        <li><a href="#aws">AWS</a></li>
        <li><a href="#gcp">GCP</a></li>
        <li><a href="#conclusion-1">Conclusion</a></li>
      </ul>
    </li>
    <li><a href="#key-insights-and-conclusions">Key insights and conclusions</a></li>
  </ul>
</nav>
    <ul>
    <li><a href="#top">Top</a></li>
    <li><a href="#end">End</a></li>
    </ul>
</div></aside>

            

            <div class="post-content">
                <p>This benchmark is done in order to help in the process of deciding which public provider to migrate to. Three public cloud offers are studied (GCP (Google Cloud Platform), AWS (Amazon Web Service) and Microsoft Azure) in comparison to the private solution.</p>
<h2 id="computing-offer-comparison">Computing offer comparison</h2>
<p>It is important to note that all the aforementioned providers have support for Linux as well as Windows VMs. Although terminologies vary widely between providers (machine type for GCP, instance for AWS, VM for Azure), they all provide a couple of essential categories of common machine families. This benchmark of computing offers will focus mainly on the spread and diversity of computing instance offers through the lens of the computing ratio variable (amount of vCPU per GiB of memory). Using different methods of dynamically fetching the different computing instance offers from the cloud providers, we examined the following family types:</p>
<ul>
<li><strong>General purpose</strong> instances provide a balance of compute, memory and networking resources, and can be used for a variety of diverse workloads. These instances are ideal for applications that use these resources in equal proportions such as web servers and code repositories and are offered by all of the aforementioned providers.</li>
<li><strong>Memory optimized</strong> are ideal for tasks that require intensive use of memory with higher memory-to-vCPU ratios than the N1 high-memory machine types. These machine types are suited for in-memory databases and in-memory analytics, such as SAP HANA and business warehousing (BW) workloads, genomics analysis, SQL analysis services, &hellip;.</li>
<li><strong>Compute-optimized</strong> machine types are ideal for compute-intensive workloads.</li>
</ul>
<p>The instance performance (vCPUs, memory(GiB)) and family type are then computed in order to generate boxplots with the preferred grouping (by cloud provider, by family, etc&hellip;).</p>
<p>
<img src="/img/bench_computing_provider.png"  class="center"  />


<p style="text-align:center; font-style: italic; padding-bottom: 15px;">Boxplot of compute ratio per cloud provider</p></p>
<p>A comparison of the computation/memory ratios between providers shows that GCP has the most diverse offers when it comes to vCpus/Memory despite having a lower amount of machine types than other providers. It is also noticeable (within the given dataset) that GCP and AWS EC2 tend to offer a higher Computation/Memory ratio than Azure whose spread is the lowest.</p>
<p>
<img src="/img/bench_computing_provider_class.png"  class="center"  />


<p style="text-align:center; font-style: italic; padding-bottom: 15px;">(a) Boxplot of compute ratio per cloud provider class</p></p>
<p>
<img src="/img/bench_computing_provider_ratio.png"  class="center"  />


<p style="text-align:center; font-style: italic; padding-bottom: 15px;">(b) Boxplot of compute ratio per cloud provider grouped by ratio</p></p>
<p>Outliers excluded, GCP General Purpose seems to offer the most variety, however their Compute optimized and Memory optimized offers are quite lacking compared to their counterparts. This is more telling of a different nomenclature between the cloud providers and does not grant us much insight into the diversity of their offers.</p>
<p>By applying the classification labels to the actual ratio, we get a more complete picture. AWS excels in compute optimized instance in comparison to other providers whereas GCP and Azure take the lead in memory optimized instances. On the other hand, with general purpose machines, AWS maintains a higher spread ratio whilst GCP mainly focuses on the lower proportion of the spectrum.</p>
<hr>
<h2 id="hosted-database-offers">Hosted database offers</h2>
<p>In this section, the different cloud providers will be used as the backbone for comparing the advantages and drawbacks of using a hosted database rather than running a self administered database cluster in k8s. Specifically a version of the vulnerability-assessment-tool database will be loaded onto a hosted database instance (with or without replication) and a self administered k8s cluster to compare their price, migration cost as well as their operational cost.</p>
<h3 id="cost">Cost</h3>
<p>In order to establish a proper price comparison, a set of different cases for resource consumptions are defined:</p>
<ul>
<li><strong>Lightweight HA</strong> : the cluster is instantiated with no prior scan and data, then, the bugs are loaded using the patch-analyzer. This deployment is not destined for high availability or resilience (therefore with less replicas, no auto-scaling) and is optimal for small testing environments with a 6 month usage buffer with the sufficient amount of replicas that will ensure high availability and resilience.</li>
<li><strong>Medium Load HA</strong> : the cluster is instantiated with no prior scan and data, then, the bugs are loaded using the patch-analyzer. This deployment is not destined for high availability or resilience (therefore with less replicas, no auto-scaling) and is optimal for small production environments with a 2 year buffer with the sufficient amount of replicas that will ensure high availability and resilience.</li>
<li><strong>Production Load HA</strong> : the cluster is loaded with the latest dump of the internal SAP vulnerability-assessment-tool database (which at the time of this document creation is around 350GB). This deployment is not destined for high availability or resilience and is optimal for production environments with a 3-5 year usage buffer. This data load includes app specific data (once those are removed, the database size is around 150GB in our current setup) with the sufficient amount of replicas that will ensure high availability and resilience.</li>
<li><strong>Hosted DB</strong> : for using a pre-existing database (for cloud providers such as GCP, AWS, Azure, etc&hellip;) which require lower resources as the database are no longer self managed.</li>
</ul>
<p>From each of these deployment case, a range of resource list is calculated; from the minimum to the maximum amount of CPU and Memory (GiB), the amount of Persistent Volumes (GiB) and the cost of the hosted database instance if required. Then using GCP cost calculator, the cost for each case is computed. Note that this methodology overestimates by a big margin the cost of computing resources (CPU, Memory) because it assumes that all machines will consume 100% of the allocated resource continuously (24/7) for one month. Mapping the appropriate deployment cases to its PVC size, the relative difference in cost between a hosted and non hosted HA deployment can be measured using the estimated cost range.</p>
<p>The price comparison is applied in two regions, eu-west-2 (one of the most expensive regions in GCE) and us-central-1 (one of the cheapest regions in GCE), to establish a possible evolution range of the difference in cost. In both cases, the hosted deployment becomes profitable over the self managed cluster when the data load surpasses 200GB.</p>
<p>
<img src="/img/bench_vulas_price_eu_west2.png"  class="center"  />


<p style="text-align:center; font-style: italic; padding-bottom: 15px;">(a) eu-west-2</p>

<img src="/img/bench_vulas_price_us_central1.png"  class="center"  />


<p style="text-align:center; font-style: italic; padding-bottom: 15px;">(a) us-central-1</p></p>
<h3 id="services">Services</h3>
<p>Unlike self managed database clusters, snapshots, backups and maintenance windows are options which can be added for most hosted database offer by cloud providers without extra operational cost (testing, developing and maintaining these scripts). However the most interesting feature is database storage auto resizing which ensures its vertical scalability. The hosted database service main drawback is that root access is not allowed, so in order to migrate from an external database, the dump has to be done without owner or privileges. This minor setback is however minuscule compared to the advantages cited previously.</p>
<h3 id="performance">Performance</h3>
<p>The performance benchmark is run on each cloud provider (each with comparable amount of resources) with the full vulnerability-assessment-tool production workload (350GB of data, without trimmed application and space related data) against a script of read-only requests (same script and pgbench parameters below.</p>
<ul>
<li><strong>GCP (helm)</strong>: using the self managed HA postgres with replication and pooling</li>
<li><strong>GCP (cloudSQL non optimised)</strong>: using the CloudSQL Postgres offer with no production configuration and only a single instance without read replicas</li>
<li><strong>GCP (cloudSQL optimised)</strong>: using the CloudSQL Postgres offer with production configuration and only a single instance without read replicas</li>
<li><strong>GCP (cloudSQL optimised)</strong>: using the CloudSQL Postgres offer with production configuration and only a single instance without read replicas and 400GB of SSD (increases IOPs drastically over the bare minimum at 257GB)</li>
<li><strong>GCP (cloudSQL optimised, 1 replica)</strong>: using the CloudSQL Postgres offer with production configuration and only a single instance with read replicas and pg_bench querying a pgpool statefulset with 3 replicas.</li>
<li><strong>AWS (helm)</strong>: using the self managed HA postgres with replication and pooling</li>
<li><strong>AWS (rds non optimised)</strong>: using the RDS PostgreSQL offer without implementing production configuration and only a single instance witout read replicas</li>
<li><strong>AWS (rds optimised)</strong>: using the RDS PostgreSQL offer with production configuration and only a single instance without read replicas</li>
<li><strong>AWS (rds optimised, 1 replica)</strong>: using the RDS Postgres offer with production configuration and only a single instance with read replicas and pg_bench querying a pgpool statefulset with 3 replicas.</li>
<li><strong>Azure (helm)</strong>: using the self managed HA postgres with replication and pooling</li>
<li><strong>Azure (hosted non optimized)</strong> : using the RDS PostgreSQL offer without implementing production configuration and only a single instance witout read replicas. However, due to the lack of possible configurations (most configuration listed below being unavailable or unchangeable), the optimized version will not be represented.</li>
</ul>
<p><strong>Reproduction steps</strong>:</p>
<ul>
<li>Create the database cluster on SAP&rsquo;s Converged Cloud based on three machines, each mounting a 400GB PVC provisioned by Openstack Cinder:
<ul>
<li>1 * 24560MB RAM, 24 VCPU, 64GB disk hosting the master database</li>
<li>2 * 16368MB RAM, 16 VCPU, 64GB disk hosting the replicas</li>
</ul>
</li>
<li>Use the previous machines as a node pool for running k8s (version 1.15.2) and a 16GiB, 8vCPUs machine from which to run the benchmarking tool.</li>
<li>Load the production database:
<ul>
<li>Total size: 354 GB</li>
<li>Tuples inserted: 412369404</li>
<li>Amount of tables: 30</li>
</ul>
</li>
<li>Run the <strong>pgbench</strong> tool as a k8s scheduled job within the cluster (on a distinct node from databases) with the following specs (Scaling factor: 1, Query mode: simple, Number of clients: 80, Number of threads: 8, Script: sample of read queries (with varying complexity) representative of the overall population of queries to the real life work load).</li>
<li><strong>production configuration</strong>:</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#66d9ef">temp_file_limit</span>: <span style="color:#ae81ff">48593190</span>
<span style="color:#66d9ef">work_mem</span>: <span style="color:#ae81ff">64</span>
<span style="color:#66d9ef">maintenance_work_mem</span>: <span style="color:#ae81ff">1024</span>
<span style="color:#66d9ef">max_connections</span>: <span style="color:#ae81ff">100</span>
<span style="color:#66d9ef">random_page_cost</span>: <span style="color:#ae81ff">2</span>
<span style="color:#66d9ef">autovacuum_max_workers</span>: <span style="color:#ae81ff">8</span>
<span style="color:#66d9ef">log_autovacuum_min_duration</span>: <span style="color:#ae81ff">1</span>
<span style="color:#66d9ef">autovacuum_analyze_scale_factor</span>: <span style="color:#ae81ff">0.15</span>
</code></pre></div><p>Note that as the performance of the Azure deployment is too poor (being the outlier in every case and making the charts non exploitable), the charts shown below do not include Azure. Due to the fact that the helm chart has been developed and optimized for Converged Cloud, its performance will likely be better than other test cases. In order to not overclutter the charts, the following colour coding convention is used:</p>
<ul>
<li><p style='color: #FBBC04; display: inline;'>AWS</p></li>
<li><p style='color: #34A853; display: inline;'>Azure</p></li>
<li><p style='color: #4285F4; display: inline;'>GCP</p></li>
</ul>
<table>
<thead>
<tr>
<th>provider</th>
<th>helm</th>
<th>hosted</th>
<th>optimised</th>
<th>replicated</th>
<th>bigssd</th>
<th>mean latency</th>
</tr>
</thead>
<tbody>
<tr>
<td>azure</td>
<td>x</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>1.9E+04</td>
</tr>
<tr>
<td>gcp</td>
<td>x</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>1.3E+04</td>
</tr>
<tr>
<td>aws</td>
<td>x</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>1.2E+04</td>
</tr>
<tr>
<td>ccloud</td>
<td>x</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>8.1E+03</td>
</tr>
<tr>
<td>gcp</td>
<td></td>
<td>x</td>
<td>x</td>
<td>x</td>
<td>x</td>
<td>1.0E+04</td>
</tr>
<tr>
<td>aws</td>
<td></td>
<td>x</td>
<td>x</td>
<td>x</td>
<td>x</td>
<td>1.2E+04</td>
</tr>
<tr>
<td>gcp</td>
<td></td>
<td>x</td>
<td>x</td>
<td>x</td>
<td></td>
<td>1.0E+04</td>
</tr>
<tr>
<td>azure</td>
<td></td>
<td>x</td>
<td>x</td>
<td>x</td>
<td></td>
<td>6.9E+04</td>
</tr>
<tr>
<td>gcp</td>
<td></td>
<td>x</td>
<td>x</td>
<td></td>
<td></td>
<td>1.0E+04</td>
</tr>
<tr>
<td>aws</td>
<td></td>
<td>x</td>
<td>x</td>
<td></td>
<td></td>
<td>1.2E+04</td>
</tr>
<tr>
<td>gcp</td>
<td></td>
<td>x</td>
<td></td>
<td></td>
<td></td>
<td>4.2E+04</td>
</tr>
<tr>
<td>aws</td>
<td></td>
<td>x</td>
<td></td>
<td></td>
<td></td>
<td>4.6E+04</td>
</tr>
<tr>
<td>azure</td>
<td></td>
<td>x</td>
<td></td>
<td></td>
<td></td>
<td>7.1E+04</td>
</tr>
</tbody>
</table>
<p style="text-align:center; font-style: italic; padding-bottom: 15px;">Average latency (ms) per deployment setup</p>
<p>It is demonstrable that using the cloud provider database as a service offer using naive configuration (without tailoring it to the production workload) is detrimental, as for AWS, GCP as well as Azure the query latency is almost 4 times the naive helm chart deployment.</p>
<p>On the other hand, with replication and tailored production configuration, the hosted database offer certain performance gain as well as price benefits (lower overhead cost as well as base cost). On the other hand, since performance can be gained using bigger SSDs (increased IOPs), one can use this to optimize the database, but an increase of 100GB in the ssd size won&rsquo;t offer much of a gain in latency (as seen by the similar outcomes for big-ssd deployments).</p>
<table>
<thead>
<tr>
<th>count</th>
<th>mean</th>
<th>std</th>
<th>min</th>
<th>max</th>
</tr>
</thead>
<tbody>
<tr>
<td>10</td>
<td>0.002609</td>
<td>0.000599</td>
<td>0.000152</td>
<td>0.002742</td>
</tr>
</tbody>
</table>
<p style="text-align:center; font-style: italic; padding-bottom: 15px;">Relative difference statistics between deployment and reference</p>
<p>These query latencies are not affected by the network latency, nor by the speed of the connection establishing due to the fact that the relative difference between tps (transaction per second) with and without handshake is relatively low: with a mean of 0.0026% and a standard deviation of 0.0005 points.</p>
<p>Grouping by complexity (arbitrary decision on the mean of all latency results for all providers), confirms the fact that GCP optimised seems to be the one whose performance in most case closest and some cases better than the CCloud native helm chart. The sample of query given do not however represent the frequency of their usage, therefore, a weighted approach (with usage $frequency \times cost$ ) would give a better approximation of the actual performance of the vulas database in each scenario. It would also be interesting to group the queries into their operation type (for instance seq_scan, index_scan, nested_loop, etc.) in order to gauge the performance of the database for each operation per cloud provider.</p>
<hr>
<h2 id="performance-benchmark">Performance benchmark</h2>
<p>In order to evaluate in an unbiased manner the performance of each cloud provider, we&rsquo;ll base ourselves on the <a href="https://github.com/GoogleCloudPlatform/PerfKitBenchmarker">PerfKitBenchmarker</a> (set of benchmarks to measure and compare cloud offerings developed and open-sourced by GoogleCloudPlatform) for Database, Networking and Instance performance and <a href="https://github.com/kubernetes/perf-tests">perf-test</a> (performance test kit for kubernetes deployments) for provided control plane performance.</p>
<h3 id="reproduction-environment">Reproduction environment</h3>
<table>
<thead>
<tr>
<th>Provider</th>
<th>Default Zone</th>
<th>Machine Type</th>
<th>vCPUs</th>
<th>Memory (GiB)</th>
</tr>
</thead>
<tbody>
<tr>
<td>GCP</td>
<td>us-central1-a</td>
<td>n1-standard-1</td>
<td>1</td>
<td>∼3.5</td>
</tr>
<tr>
<td>AWS</td>
<td>us-east-1a</td>
<td>m3.medium</td>
<td>1</td>
<td>3.75</td>
</tr>
<tr>
<td>Azure</td>
<td>eastus2</td>
<td>StandardA1</td>
<td>1</td>
<td>2.1</td>
</tr>
</tbody>
</table>
<p style="text-align:center; font-style: italic; padding-bottom: 15px;">Default reproduction environment for PerfKitBenchmarker</p>
<p>Note that the previous machine types may vary slightly depending on the test applied (due to different resource requirements). The default machine type is meant to be as close together as possible, however Azure&rsquo;s machine type is a bit less performant than the others, which might disadvantage it. AWS should however profit from this disparity during the benchmarks.</p>
<p>On the other hand, all regions tested are within the US, so comparable results should be observed and the regional factor should not influence the results.</p>
<h3 id="computing-instance">Computing instance</h3>
<p>The cloud provider instance performance can be evaluated thanks to aggregating data from the unixbench benchmark group which cover tests ranging from computing, pipe performance, system calls, to unix file copies.</p>
<h4 id="basic-arithmetics">Basic arithmetics</h4>
<p>Drystone is a synthetic benchmark application meant to mimic a real life application with a mix of mathematical and other operations (with the following specifications: Integer performance predominated, with little or no floating-point calculations, and could be contained inside small memory subsystems). This benchmark, although lightweight and quite portable, is susceptible to compiler optimization interference (as it is written in C) and only captures a tiny fraction of mathematical and basic operations. Because these inherent flaws are also present in the Wheststone test, data from these benchmarks are excluded.</p>
<h4 id="pipe-throughput">Pipe throughput</h4>
<p>This test (composed of two tests Pipe throughput and Pipe-based Context Switching) intends to evaluate pipe performances (A pipe being the simplest form of communication between processes) by measuring the amount of lines executed per seconds (lps).</p>

<img src="/img/bench_pipe_throughput.svg"  class="center"  />


<table>
<thead>
<tr>
<th>provider</th>
<th>pipe throughput(lps)</th>
<th>pipe based context-switching (lps)</th>
<th>relative performance ratio (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>aws</td>
<td>409657</td>
<td>73558.4</td>
<td>17.96</td>
</tr>
<tr>
<td>azure</td>
<td>366060.6</td>
<td>95920.7</td>
<td>26.20</td>
</tr>
<tr>
<td>gcp</td>
<td>1097697</td>
<td>137761.1</td>
<td>12.55</td>
</tr>
</tbody>
</table>
<p>Pipe throughput is the number of times (per second) a process can write 512 bytes to a pipe and read them back. The pipe throughput test has no real counterpart in real-world programming. The context switching measures the number of times two processes can exchange an increasing integer through a pipe.</p>
<p>The pipe-based context switching test is more like a real-world application. The test program spawns a child process with which it carries on a bi-directional pipe conversation. The relative difference between pipe throughput and pipe based context switching (due to added the added complexity) is minimized in GCP&rsquo;s case.</p>
<h4 id="shell-and-process-calls">Shell and process calls</h4>
<p>The process creation test measures the number of times a process can fork and reap a child that immediately exits. Process creation refers to actually creating process control blocks and memory allocations for new processes, so this applies directly to memory bandwidth. Typically, this benchmark would be used to compare various implementations of operating system process creation calls. The shells scripts test measures the number of times per minute a process can start and reap a set of one and eight concurrent copies of shell script where the shell script applies a series of transformation to a data file. Due to shell scripts extra complexity, it is expected that the performance would be lower that the pure process creation.</p>

<img src="/img/bench_shell.svg"  class="center"  />


<table>
<thead>
<tr>
<th></th>
<th>aws</th>
<th>azure</th>
<th>gcp</th>
</tr>
</thead>
<tbody>
<tr>
<td>process creation (lps)</td>
<td>4880.4</td>
<td>4647.4</td>
<td>110701</td>
</tr>
<tr>
<td>concurentshell (lps)</td>
<td>3339.7</td>
<td>3440.4</td>
<td>7771.78</td>
</tr>
<tr>
<td>concurrentshell (lps)</td>
<td>435</td>
<td>426.4</td>
<td>999.2</td>
</tr>
<tr>
<td>performance lost from shell overhead (%)</td>
<td>-86.97</td>
<td>-87.60</td>
<td>-87.14</td>
</tr>
<tr>
<td>performance lost from concurrent shell (%)</td>
<td>-31.57</td>
<td>-25.97</td>
<td>-29.79</td>
</tr>
</tbody>
</table>
<p>Concurrent shell execution seems to be a road block on all providers as the execution speed in lpm (lines per minute) drop significantly (at the same ratio as the amount of concurrent executions). This however comes from the Linux kernel implementation as well as mutex locks on the data file and not from the cloud providers themselves. The performance drop due to concurrent execution is quite similar all throughout providers; the 8 concurrent shell execution having around 12% to 13% of process number finished of the single shell execution. The same observation can be made for the shell overhead. Despite the similar relative drops, the base process call performance of GCP remains the highest which, in turn, translates to their instances being the most optimized for shell and process calls.</p>
<h4 id="conclusion">Conclusion</h4>
<p>Instance performances measured through the unix-bench kit show that GCP instances are the most optimized with AWS and Azure following closely behind. This difference possibly comes from different implementation of the base image; GCP using a <em>Container Optimized OS</em> based on Chromium OS, AWS using <em>Amazon Linux AMI</em> and Azure using the <em>Canonical Ubuntu</em>. However, this is mere speculations because the design documents for these images do not go into detail about their implementation.</p>
<hr>
<h2 id="databases-and-filesystem-performance">Databases and filesystem performance</h2>
<h3 id="fio--io-speed">Fio — I/O speed</h3>
<p>Fio is short for Flexible IO, a versatile IO workload generator. In this benchmark, each provider provision one equivalent machine type and performs a series of sequential or random operations such as reads or writes of a $10 \times 10 \times 1000$ mb file with a given block size in bytes (kilobytes) and a given iodepth (Number of IO operations is issues to the OS at a given time). This would give an idea of IOPs for each provider and the underlying performance of their file system. In detail, the disk access pattern benchmarked are the following:</p>
<ul>
<li><strong>Random read</strong>: blocks of 4kb are read from random locations on the surface of the device at an IOdepth of 1. This access pattern is quite common during operating system startup, where lots of configuration and driver files must be read from the disk and can therefore give an insight into instance startup or reboot.</li>
<li><strong>Random write</strong>: blocks of 512kb are written from random locations on the surface of the device at an IOdepth of 1. This type of access pattern is common when running applications from a disk and also when copying large application directories.</li>
<li><strong>Sequential read</strong>: contiguous blocks of 4kb are read/write from adjacent locations on the surface of the device at an IOdepth of 1. This type of access pattern is common when reading large files.</li>
<li><strong>Sequential write</strong>: contiguous blocks of 512kb are read/write from adjacent locations on the surface of the device at an IOdepth of 1. This type of access pattern is common when writing large files.</li>
<li><strong>Random Parallel read</strong>: the fio process will try to submit 64 simultaneous I/O, each attempting to random write a 4kb blocksize.</li>
</ul>
<p>With the above job config, fio gathers two metrics <strong>IOps</strong> and <strong>Latency per operation</strong>. The latter is used because it is richer, giving access to each percentile of the latency distribution.</p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center"></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">
<img src="/img/bench_fio_random_read.svg"  class="center"  />

<p style="text-align:center; font-style: italic; padding-bottom: 15px;">(a) random read</p></td>
<td align="center">
<img src="/img/bench_fio_sequential_read.svg"  class="center"  />

<p style="text-align:center; font-style: italic; padding-bottom: 15px;">(b) sequential read</p></td>
</tr>
<tr>
<td align="center">
<img src="/img/bench_fio_random_write.svg"  class="center"  />

<p style="text-align:center; font-style: italic; padding-bottom: 15px;">(c) random write</p></td>
<td align="center">
<img src="/img/bench_fio_sequential_write.svg"  class="center"  />

<p style="text-align:center; font-style: italic; padding-bottom: 15px;">(d) sequential write</p></td>
</tr>
</tbody>
</table>
<p>
<img src="/img/bench_fio_random_parallel_read.svg"  class="center"  />

<p style="text-align:center; font-style: italic; padding-bottom: 15px;">(e) random parallel read</p></p>
<p>From this point onwards, two criterias are measured; <strong>Performance</strong> (characterized by the median latency ($p50$)) and <strong>Stability</strong> (characterized by the standard deviation of the sample).</p>
<p><strong>Write operations</strong></p>
<p>For random writes, all instances offer stable performance with a homogeneous percentile distribution (the difference between the 1st and 90-th percentile is relatively close). GCP excels in this category with a the lowest median (502 $\mu$s) and p95 (1608 $\mu$s).</p>
<p>For sequential writes, Azure takes the lead with a median at 31104$\mu$s. However, both AWS and Azure suffer from stability (with standard deviations over 7 times bigger that GCPs) whereas GCP&rsquo;s is more stable with the highest median of around 444416$\mu$s.</p>
<p><strong>Read operations</strong></p>
<p>When it comes to sequential reads, Azure takes the lead on performance but GCP overtakes it on stability. As for random reads, AWS is the favoured in both those categories. Finally, for random parallel reads, AWS beats both other providers for performance (median at 18560 $\mu$s) with gcp acing stability (with the lowest standard deviation).</p>
<p><strong>Conclusion</strong></p>
<p>It is important to note that random access seems to be outperforming sequential ones in both operation types, which is impossible whether it is in the case of an HDD (time to seek to random segments of the disk should add up to a higher latency) or of an SSD (the overhead cost of initiating IO operations should make random access less performant). This observation can possibly be explained in part due to the fact that the total size of the file written to disk exceeds the default SSD size attached to the machine which would then require scaling up or adding a PV, causing the higher latency.</p>
<p>If the following hypothesis were true, we can confirm the observation that none of the provider file system performance outperforms each other in all disk access pattern. And that for overall file system performance, one should choose AWS and for stability GCP.</p>
<h3 id="redis--key-value-data-stores">Redis — key value data stores</h3>
<p>The redis benchmark is based on creating equivalent computing instances (VMs in Azure, EC2 instance in AWS and Machines in GCP) running a single node redis database and querying it (with update, read and insert operations) from multiple computing instances each running a specific amount of threads (the amount of threads being the varying parameter between metric measurement). As redis is an open source (BSD licensed), in-memory data structure store, used as a database, cache and message broker, this benchmark would also give an idea as to the efficiency of in memory storage in each of the respective provider computing instances.</p>
<p>We mainly base our comparison of redis performance on throughput and latency. As those two are closedly tied together, the data collected mainly focuses on the former which is the number of operations processed by the database within a given period of time. Graphing the throughput evolution per concurrent server (redis) threads and fitting it to a function would give an idea of the performance for redis that can be gained from vertical scaling. Fitting the distribution with a logarithmic function works perfectly with AWS and Azure with a low coefficient of determination ($R^2 \geq 0.97$) but not well with GCP.</p>
<p>
<img src="/img/bench_redis_throughput.svg"  class="center"  />


<p style="text-align:center; font-style: italic; padding-bottom: 15px;">Redis throughput (req/s) per number of server threads</p></p>
<ul>
<li>aws ($x : 7312 \times ln(x) + 1433 $ and $R^2 = 0.971$)</li>
<li>azure ($x : 6476 \times ln(x) - 2038 $ and $R^2 = 0.981$)</li>
<li>gcp ($x : 17195 \times ln(x) + 33993 $ and $R^2 = 0.729$)&rdquo;</li>
</ul>
<p>Each provider reaches an optimal throughput after a certain point and the growth is then stunted. This plateau occurs at $100 000$ requests/sec for GCP, $40 000$ requests/sec for AWS and $35 000$ requests/sec for Azure. GCP is therefore the most fitted for redis database hosting.</p>
<h3 id="mongodb--document-oriented-databases">MongoDB — document oriented databases</h3>
<p>MongoDB is a general purpose, document-based, distributed database. The mongo <a href="https://github.com/brianfrankcooper/YCSB">YCSB benchmark</a> (Yahoo! Cloud Serving Benchmark) is meant to provide a well rounded picture of a system’s performance using in our case a database with 1000000 records, with an equivalent client machine performing 10000 operations with 32 clients with a varying proportion of read, insert and update and 1 server machine serving with 32 threads.</p>
<p>Using the percentile distribution of the latency (in ms, scaled down to logarithmic scale) for certain CRUD operations (read, insert, update without bulk or delete operations) in Mongo yields the following graph:</p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center"></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">
<img src="/img/bench_mongo_insert.svg"  class="center"  />

<p style="text-align:center; font-style: italic; padding-bottom: 15px;">(a) insert</p></td>
<td align="center">
<img src="/img/bench_mongo_read.svg"  class="center"  />

<p style="text-align:center; font-style: italic; padding-bottom: 15px;">(b) read</p></td>
</tr>
</tbody>
</table>
<p>
<img src="/img/bench_mongo_update.svg"  class="center"  />

<p style="text-align:center; font-style: italic; padding-bottom: 15px;">(c) update</p></p>
<p style="text-align:center; font-style: italic; padding-bottom: 15px;">Write latency (ms)</p>
<p>Read operation latency is unsurprisingly the lowest in all providers with GCP latency quantiles being consistently the lowest. As Insert operations will add a document to a collection if it does not exist, it is in general more expensive, having to both read for document&rsquo;s existence and write to storage. The previous observation regarding the GCP deployment&rsquo;s performance is similarly observed in (c).</p>
<p>Finally, Updates correspond to modifying an existing document which entails reading the database for the document, then if it exists, seek the memory address corresponding to the modified field and access it to overwrite the old value. This extra cost is reflected in all providers as their medians exceed that of any other operations. Although AWS&rsquo;s median latency is the lowest, from the 75-th percentile onward the operation explodes exponentially in cost, making it less viable than GCP.</p>
<p>In conclusion, GCP is the provider of choice for deployment a Mongodb database. It would however be interesting to include more data regarding different composition operations and add the cluster size as a parameter (the above only relevant to a single cluster mongo).</p>
<h3 id="cassandra--column-oriented-databases">Cassandra — column oriented databases</h3>
<p>Cassandra is a distributed, column oriented database designed for high data loads and high availability through clustering. Implemented in Java, evaluating its performance would not only give insight into the performance of an optimized Java application but also the networking throughput between nodes of the cluster.</p>
<p>The test runs <strong>cassandra-stress</strong> (stress test utility maintained by the cassandra team themselves, and thus, we can suppose that it is agnostic as to the underlying provider) on a three node cassandra cluster hosted on three instantiated machines within the same sub region of the cloud provider on the QUORUM consistency level (A write must be written to the commit log and memtable on a quorum of replica nodes across all data centers).  The data loaded corresponds to the one proposed by the Cassandra YCSB test, consisting of $2000000$ partitions stored in a single keyspace (a number of partitions corresponding to huge applications, which is the intended use case of cassandra).</p>
<table>
<thead>
<tr>
<th>provider</th>
<th>mean (ms)</th>
<th>median (ms)</th>
<th>p95 (ms)</th>
<th>max (ms)</th>
</tr>
</thead>
<tbody>
<tr>
<td>aws</td>
<td>34.7</td>
<td>25.4</td>
<td>82.8</td>
<td>824.3</td>
</tr>
<tr>
<td>azure</td>
<td>34.5</td>
<td>20.5</td>
<td>93.7</td>
<td>475.7</td>
</tr>
<tr>
<td>gcp</td>
<td>16.4</td>
<td>26.4</td>
<td>58.4</td>
<td>8191.9</td>
</tr>
</tbody>
</table>
<p style="text-align:center; font-style: italic; padding-bottom: 15px;">Latency statistics for cassandra mixed workload</p>
<p>With the previously mentioned workload, a common trend can be observed in all cloud providers that the latency distribution is positive-skewed with the mean being significantly higher than the median due to high outlier values. In both mean and median, GCP wins out over the others despite an extremely high max latency.</p>
<hr>
<h2 id="networking">Networking</h2>
<h3 id="instance-throughput">Instance throughput</h3>
<p>Iperf is a tool for active measurements of the maximum achievable bandwidth on IP networks. This benchmark creates two equivalent machines, each allocated an external IP that communicates with each other through these IPs (IPv4 to be precise) for 60s.</p>
<p>Measuring the throughput (the amount of packets that can be passed through the virtual/physical networking layer - an extremely rich metric that can be affected by medium limitations (underlying networking equipment data rate), congestion (and congestion handling), protocol, etc&hellip;) of the previous operations on multiple iterations will give us an insight into the real networking performance of instances and compare them to the advertised bandwith (the theoretical throughput guaranteed by each vendor).</p>
<table>
<thead>
<tr>
<th>provider</th>
<th>Mean throughput (Mbits/sec)</th>
<th>Stddev throughput (Mbits/sec)</th>
</tr>
</thead>
<tbody>
<tr>
<td>aws</td>
<td>301.5</td>
<td>10.595</td>
</tr>
<tr>
<td>azure</td>
<td>501.75</td>
<td>0.433</td>
</tr>
<tr>
<td>gcp</td>
<td>1967.25</td>
<td>2.586</td>
</tr>
</tbody>
</table>
<p style="text-align:center; font-style: italic; padding-bottom: 15px;">TCP IPv4 throughput (Mbits/sec) per provider</p>
<p>Due to relatively low standard deviations observed and the high amount of iterations, it can be remarked that GCP instances seem to have the highest throughput with the other providers seriously lacking behind. However, an appropriate measure of the bandwidth is not standardized (each provider uses his own metric for bandwith measurement, GCP publishes Network egress bandwith (Gb/s), Azure publishes the Max NICS (Network interface controller) and AWS qualifies network performance as Low, Moderate or High without further elaboration) accross cloud platforms nor is it public, thus, making a comparison between network throughout and bandwidth impossible.</p>
<h3 id="webserver-latency">Webserver latency</h3>
<p>
<img src="/img/bench_nginx.svg"  class="center"  />

<p style="text-align:center; font-style: italic; padding-bottom: 15px;">Latency percentile (log ms) distribution for NGINX</p></p>
<p>NGINX is a web server which can also be used as a reverse proxy, load balancer, mail proxy and HTTP cache. This benchmark runs an NGINX server instance on an equivalent machine with 1 cpu and 1 thread and another exactly similar machine running as a client within the same region. The client runs a wrk2 tool with a sampling interval of 100msec for a duration of 60s, the results from which a latency percentile distribution is extracted. This doubles as a proof of concept as to the theoretical performance of the nginx ingress controller (purely hypothetical as the nginx ingress controller uses a modified image of NGINX with more overhead) used in the vulnerability-assessment-tool-admin helm chart.</p>
<p>From the results, GCP consistently outperforms other vendors with the lowest latency in all measured latency percentiles. It is important to note that the observed criteria does not cover the whole spectrum of possible deployments; multiple CPUs (which directly influence the <a href="https://www.nginx.com/blog/testing-the-performance-of-nginx-and-nginx-plus-web-servers/">reverse proxy&rsquo;s throughput</a>), SSL serving, caching efficiency, etc&hellip; The lack of benchmarking environment disclosure by the PerfKitBenchmarker tool rears its ugly head once again as the test conditions have to be extracted from the source code.</p>
<h3 id="k8s-cluster-dns-resolution">k8s Cluster DNS resolution</h3>
<p>Evaluating the performance of the hosted control plane is a relatively tough task due to the multiple variables that need to be accounted for (k8s version, base image and specs for the node pool, security and monitoring options that all affect performance). Using the <a href="https://github.com/kubernetes/perf-tests">perf-tests</a> toolkit gives either resource heavy tests (some even requiring to reach the k8s design limit for nodes in the cluster, around 100) or inconclusive tests.</p>
<p>The test to quantify the DNS name resolution latency accross was inconclusive as the precision of the data set collected is too low to distinguish between cloud platforms. Because of either the lack of precision in the data gathered or the extremely similar cluster configuration defined, all providers&rsquo;s k8s control plane display the same latency. However, despite this, we discovered that GCP only deploys a KubeDNS server whereas the other two deploy both a CoreDNS and a KubeDNS server. Although both are DNS servers, CoreDNS (introduced in k8s as a General Availability and replaced kubeDNS as the default name resolution server) offer better performance for external domain name resolution and the latter for internal name resolution. This design/technical choice may come from Google Public DNS&rsquo;s dominance in the Internet space and GCP being in the same networking abstraction as the latter.</p>
<hr>
<h2 id="ease-of-automation-and-tooling">Ease of Automation and Tooling</h2>
<p>Unlike previous sections, assessing the ease of automation and tooling for each platform is purely subjective and strictly based on the appropriate use case. As such, from this point, this work assumes the deployment previously described scenario; a k8s hosted control plane connected to a hosted database via a VPC bridge.</p>
<h3 id="azure">Azure</h3>
<p>The azure terraform modules is loosely divided into three providers:</p>
<ul>
<li>Azure: used to configure infrastructure in Microsoft Azure</li>
<li>Azure Stack: used to manage resources in Azure Stack</li>
<li>Azure Active Directory: used to configure infrastructure in the Azure Active directory</li>
</ul>
<p>The quality of the documentation for the Azure terraform modules are very poor, with object description being quite lacking and dispersed throughout three different providers whose distinctions and use cases are never explained in any documentation. This partially reflects Azure&rsquo;s preference for UI based approach to infrastructure provisioning over &ldquo;automation&rdquo;.</p>
<h3 id="aws">AWS</h3>
<p>Unlike Azure, AWS&rsquo;s terraform modules are centralized in an unique provider titled aws. In order for terraform to gain resource creation privileges, a service access key must be passed (safest method as hard coded credentials in terraform files are displayed as plain text) through a set of given environment variables.</p>
<p>AWS also maintains open source modules that abstract the complexity of the base objects. For instance, the terraform-aws-vpc library simplifies a VPC creation to a single declaration block in place of around 10 base object declaration blocks using the &ldquo;native&rdquo; module. However due to upstream implementation issues with AWS&rsquo;s APIs, these libraries inherited a flaw which renders infrastructure teardown impossible. As such, external tools such as <a href="https://github.com/gruntwork-io/cloud-nuke">cloud-nuke</a> are mandatory in order to clean up these failed destruction attempts.</p>
<p>Unlike EKS or RDS, no prebuilt module exist for creating the bridge mentioned. Unlike GCP, little to no examples or documentation are available to guide the provisioning of this architecture. This ultimately boils down to creating the <em>aws_vpc_peering_connection</em> object with the <em>auto_accept</em> argument set to True after the following objects are created:</p>
<ul>
<li>EKS cluster with its own VPC (<em>EKS_VPC_ID</em>)</li>
<li>RDS (Postgres) with its own VPC (<em>RDS_VPC_ID</em>})</li>
<li>Peering connection initiated from the RDS VPC to the EKS VPC</li>
<li>Route table for public subnets of EKS cluster is updated: route with destination <em>RDS_VPC_ID</em> and target peer connection from step #3 is added.</li>
<li>Route table for the RDS is updated: route with destination <em>EKS_VPC_ID</em> and target peer connection from step #3 is added.</li>
<li>The RDS security group is updated, new inbound rule is added: all traffic from <em>EKS_VPC_ID</em> is allowed</li>
</ul>
<h3 id="gcp">GCP</h3>
<p>Similarly to AWS, GCP&rsquo;s module are centralized and accessible by passing a JSON file storing the credentials. There are no clear line as to which method of credentials storing (env variables or filesystem) is more secure so terraform secret management remain contentious for every provider.</p>
<p>The CloudSQL (hosted database) automated creation is one of the most tricky object in the GCP terraform module, although still a mile ahead of AWS when it comes to simplicity. Connection the GKE cluster to the CloudSQL database requires a private IP address within the same VPC as the cluster.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-hcl" data-lang="hcl"><span style="color:#66d9ef">resource</span> <span style="color:#e6db74">&#34;google_compute_global_address&#34; &#34;private_ip_address&#34;</span> {
  name          <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;private-ip-address&#34;</span>
  purpose       <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;VPC_PEERING&#34;</span>
  address_type  <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;INTERNAL&#34;</span>
  prefix_length <span style="color:#f92672">=</span> <span style="color:#ae81ff">16</span>
  network       <span style="color:#f92672">=</span> <span style="color:#66d9ef">data</span>.<span style="color:#66d9ef">google_compute_network</span>.<span style="color:#66d9ef">default</span>.<span style="color:#66d9ef">self_link</span>
}

<span style="color:#66d9ef">resource</span> <span style="color:#e6db74">&#34;google_service_networking_connection&#34; &#34;private_vpc_connection&#34;</span> {
  network                 <span style="color:#f92672">=</span> <span style="color:#66d9ef">data</span>.<span style="color:#66d9ef">google_compute_network</span>.<span style="color:#66d9ef">default</span>.<span style="color:#66d9ef">self_link</span>
  service                 <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;servicenetworking.googleapis.com&#34;</span>
  reserved_peering_ranges <span style="color:#f92672">=</span> [
      <span style="color:#66d9ef">google_compute_global_address</span>.<span style="color:#66d9ef">private_ip_address</span>.<span style="color:#66d9ef">name</span>
  ]
}
</code></pre></div><p style="text-align:center; font-style: italic; padding-bottom: 15px;">GCP GKE-CloudSQL peering terraform declaration</p>
<p>The declaration heavily simplifies the AWS creation steps previously mentioned whilst keeping the peering logic clean and human readable.</p>
<h3 id="conclusion-1">Conclusion</h3>
<p>This small test case using terraform, although not representative of the whole image when it comes to automation on each platform, shows that for ease of usage GCP wins over the competition and thanks to the richness (with complexity being the side effect) of its object, AWS is more suited for larger infrastructure and stricter security enforcement.</p>
<hr>
<h2 id="key-insights-and-conclusions">Key insights and conclusions</h2>
<p>When it comes to both overall performance (instance, database, networking) as well as automation intuitiveness and easiness observed through the previous benchmark, GCP edges out its competitors. AWS, on the other, despite its flaws with terraform automation makes up for it in more granular infrastructure building blocks as well as filesystem efficiency. The observation resulting from the PerfKitBenchmarker should however be taking with a grain of salt due to the inherent defects of the tests that compose the kit (lack of transparent environments, machine type biases, etc&hellip;). These results are collaborated by the ones collected by the <a href="https://www.cockroachlabs.com/guides/2020-cloud-report/">Cockroach Lab 2020 Cloud Report</a> shows that GCP edges out its competitions on performance but also notes that the cloud providers performance are bound to constant fluctations as they adopt new hardware, new infrastructures and new processes.</p>
<p>Another key insight obtained is the cost and performance gained from using the hosted database as a service of the given cloud vendor for larger dataset which highlights the advantages of standardizing deployments across security research and testing projects.</p>

            </div>
        </article>
        <h2 id="end"></h2>
        <hr />

        <div class="post-info">
  				<p>
  					<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg><span class="tag"><a href="https://ichbinfrog.github.io/tags/perfkitbenchmarker">PerfKitBenchmarker</a></span><span class="tag"><a href="https://ichbinfrog.github.io/tags/gcp">GCP</a></span><span class="tag"><a href="https://ichbinfrog.github.io/tags/aws">AWS</a></span><span class="tag"><a href="https://ichbinfrog.github.io/tags/azure">Azure</a></span><span class="tag"><a href="https://ichbinfrog.github.io/tags/database">Database</a></span><span class="tag"><a href="https://ichbinfrog.github.io/tags/networking">Networking</a></span><span class="tag"><a href="https://ichbinfrog.github.io/tags/automation">Automation</a></span>
  				</p>
  		</div>
    </main>

            </div>

            
                <footer class="footer">
    <div class="footer__inner">
        <div class="footer__content">
            <span>&copy; 2020</span>
            
            <span><a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0</a></span>
        </div>
    </div>
    <div class="footer__inner">
        <div class="footer__content">
            <span><a href="https://themes.gohugo.io/hugo-theme-hello-friend-ng/">hello-friend-ng</a> Theme for <a href="http://gohugo.io">Hugo</a> by <a href="https://github.com/rhazdon">Djordje Atlialp</a></span>
        </div>
    </div>
</footer>

            
        </div>

        




<script type="text/javascript" src="/bundle.min.dc716e9092c9820b77f96da294d0120aeeb189b5bcea9752309ebea27fd53bbe6b13cffb2aca8ecf32525647ceb7001f76091de4199ac5a3caa432c070247f5b.js" integrity="sha512-3HFukJLJggt3&#43;W2ilNASCu6xibW86pdSMJ6&#43;on/VO75rE8/7KsqOzzJSVkfOtwAfdgkd5BmaxaPKpDLAcCR/Ww=="></script>



    </body>
</html>
