<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="">
<meta name="description" content="Classification Analyse exploratoire L‚Äôobjectif de ce travail est de proposer le meilleur classifieur possible √† partir de 500 observations, capable de distinguer trois classes (not√© 1, 2 et 3) de proportion initiale in√©gale ($œÄ_1 = 0.18, œÄ_2 = 0.442, œÄ_3 = 0.378$). Comme le jeu de donn√©es est synth√©tique, il est impossible d‚Äôattribuer une signification √† chaque variable. Tout de m√™me, les variables se distinguent clairement en trois groupes:
X1 √† X20, suivant une distribution bimodale continue d‚Äôesp√©rance E[X] = 5 et de variance relativement faible:" />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="https://ichbinfrog.github.io/university/sy19/" />
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$']],
            processEscapes: true,
            processEnvironments: true,
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            TeX: {
                equationNumbers: { autoNumber: "AMS" },
                extensions: ["AMSmath.js", "AMSsymbols.js"]
            }
        }
    });
    MathJax.Hub.Queue(function () {
        
        
        
        var all = MathJax.Hub.getAllJax(), i;
        for (i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });

    MathJax.Hub.Config({
        
        TeX: { equationNumbers: { autoNumber: "AMS" } }
    });
</script>


    <title>
        
            üá´üá∑ Etude d&#39;un dataset al√©atoire :: Hoang Quoc Trung 
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="/main.min.cbd9ce6c3a26d6c4a5eb4b8834c18d83518654bd8bac1fc8160486f336181c9a.css">


    
        <link rel="stylesheet" type="text/css" href="/css/post.css">
    





<meta itemprop="name" content="üá´üá∑ Etude d&#39;un dataset al√©atoire">
<meta itemprop="description" content="Classification Analyse exploratoire L‚Äôobjectif de ce travail est de proposer le meilleur classifieur possible √† partir de 500 observations, capable de distinguer trois classes (not√© 1, 2 et 3) de proportion initiale in√©gale ($œÄ_1 = 0.18, œÄ_2 = 0.442, œÄ_3 = 0.378$). Comme le jeu de donn√©es est synth√©tique, il est impossible d‚Äôattribuer une signification √† chaque variable. Tout de m√™me, les variables se distinguent clairement en trois groupes:
X1 √† X20, suivant une distribution bimodale continue d‚Äôesp√©rance E[X] = 5 et de variance relativement faible:">
<meta itemprop="datePublished" content="2020-11-28T18:16:46&#43;01:00" />
<meta itemprop="dateModified" content="2020-11-28T18:16:46&#43;01:00" />
<meta itemprop="wordCount" content="2248">
<meta itemprop="image" content="https://ichbinfrog.github.io/"/>



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://ichbinfrog.github.io/"/>

<meta name="twitter:title" content="üá´üá∑ Etude d&#39;un dataset al√©atoire"/>
<meta name="twitter:description" content="Classification Analyse exploratoire L‚Äôobjectif de ce travail est de proposer le meilleur classifieur possible √† partir de 500 observations, capable de distinguer trois classes (not√© 1, 2 et 3) de proportion initiale in√©gale ($œÄ_1 = 0.18, œÄ_2 = 0.442, œÄ_3 = 0.378$). Comme le jeu de donn√©es est synth√©tique, il est impossible d‚Äôattribuer une signification √† chaque variable. Tout de m√™me, les variables se distinguent clairement en trois groupes:
X1 √† X20, suivant une distribution bimodale continue d‚Äôesp√©rance E[X] = 5 et de variance relativement faible:"/>



    <meta property="og:title" content="üá´üá∑ Etude d&#39;un dataset al√©atoire" />
<meta property="og:description" content="Classification Analyse exploratoire L‚Äôobjectif de ce travail est de proposer le meilleur classifieur possible √† partir de 500 observations, capable de distinguer trois classes (not√© 1, 2 et 3) de proportion initiale in√©gale ($œÄ_1 = 0.18, œÄ_2 = 0.442, œÄ_3 = 0.378$). Comme le jeu de donn√©es est synth√©tique, il est impossible d‚Äôattribuer une signification √† chaque variable. Tout de m√™me, les variables se distinguent clairement en trois groupes:
X1 √† X20, suivant une distribution bimodale continue d‚Äôesp√©rance E[X] = 5 et de variance relativement faible:" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ichbinfrog.github.io/university/sy19/" />
<meta property="og:image" content="https://ichbinfrog.github.io/"/>
<meta property="article:published_time" content="2020-11-28T18:16:46+01:00" />
<meta property="article:modified_time" content="2020-11-28T18:16:46+01:00" /><meta property="og:site_name" content="Hoang Quoc Trung" />






    <meta property="article:published_time" content="2020-11-28 18:16:46 &#43;0100 &#43;0100" />








    </head>

    <body class="">
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text">ichbinfrog</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="https://ichbinfrog.github.io/projects">0_projects</a></li><li><a href="https://ichbinfrog.github.io/internships">1_internships</a></li><li><a href="https://ichbinfrog.github.io/university">2_uni_works</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            
            <span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>
</span>
        </span>
    </span>
</header>


            <div class="content">
                
    <main class="post">

        <div class="post-info">
            
            </p>
        </div>

        <article>
            <h2 class="post-title" id="top"><a href="https://ichbinfrog.github.io/university/sy19/">üá´üá∑ Etude d&rsquo;un dataset al√©atoire</a></h2>
            <aside id="toc"><style>
.toc {
    position: fixed;
    top: 50%;
    left: 2%;
    width: 20%;
    transform: translateY(-50%);
    border-radius: 5px;
    padding-bottom: 1rem;
}

.toc a {
    text-decoration: none;
    color: gray;
}

.toc a:hover {
    opacity: 0.5;
}

.toc ul {
    list-style: ">  ";
}

.toc-title {
    text-decoration: underline;
}
</style>

<div class="toc">
    <div class="toc-title">Table of Contents</div>
    <nav id="TableOfContents">
  <ul>
    <li><a href="#analyse-exploratoire">Analyse exploratoire</a></li>
    <li><a href="#feature-selection">Feature selection</a></li>
    <li><a href="#knn">KNN</a></li>
    <li><a href="#analyse-discriminante">Analyse discriminante</a>
      <ul>
        <li><a href="#r√©gularisation">R√©gularisation</a></li>
        <li><a href="#stepwise-selection">Stepwise selection</a></li>
      </ul>
    </li>
    <li><a href="#r√©gression-logistique">R√©gression Logistique</a></li>
    <li><a href="#arbres-et-for√™ts-al√©atoires">Arbres et for√™ts al√©atoires</a></li>
    <li><a href="#conclusion">Conclusion</a></li>
  </ul>

  <ul>
    <li><a href="#feature-selection-1">Feature selection</a></li>
    <li><a href="#knn-1">KNN</a></li>
    <li><a href="#mod√®les-lin√©aires">Mod√®les lin√©aires</a>
      <ul>
        <li><a href="#subset-selection">Subset selection</a></li>
        <li><a href="#r√©gularisation-1">R√©gularisation</a></li>
        <li><a href="#splines-et-gam">Splines et GAM</a></li>
      </ul>
    </li>
    <li><a href="#conclusion-1">Conclusion</a></li>
  </ul>
</nav>
    <ul>
    <li><a href="#top">Top</a></li>
    <li><a href="#end">End</a></li>
    </ul>
</div></aside>

            

            <div class="post-content">
                <h1 id="classification">Classification</h1>
<h2 id="analyse-exploratoire">Analyse exploratoire</h2>
<p>L‚Äôobjectif de ce travail est de proposer le meilleur classifieur possible √† partir de 500 observations, capable
de distinguer trois classes (not√© 1, 2 et 3) de proportion initiale in√©gale ($œÄ_1 = 0.18, œÄ_2 = 0.442, œÄ_3 = 0.378$).
Comme le jeu de donn√©es est synth√©tique, il est impossible d‚Äôattribuer une signification √† chaque variable.
Tout de m√™me, les variables se distinguent clairement en trois groupes:</p>
<p>X1 √† X20, suivant une distribution bimodale continue d‚Äôesp√©rance E[X] = 5 et de variance relativement
faible:</p>
<ul>
<li>X21 √† X45, suivant une loi normale continue d‚Äôesp√©rance E[X] = 0 et de variance elev√©e</li>
<li>X46 √† X50, avec une distribution discr√®te positive centr√©e aux alentours de E[X] = 3.7</li>
</ul>

<img src="/img/sy19_prelim.png"  class="center"  />


<h2 id="feature-selection">Feature selection</h2>
<p>Puisque nous avons une quantit√© importante de variable par rapport aux nombres d‚Äôobservations (1/10),
il serait judicieux d‚Äôappliquer des m√©thodes de r√©duction de dimensions si possible.Comme l‚Äôanalyse en
composantes principales ne fonctionne qu‚Äôavec les donn√©es quantitatives, nous posons l‚Äôhypoth√®se que les
variables X45 ‚àí X50 sont quantitatives.
Suite √† cette analyse, nous observons qu‚Äôil faut garder 40 variables afin de pr√©server 90% de la variance
expliqu√©e. Il y a donc peu d‚Äôint√©r√™t d‚Äôappliquer cette r√©duction.</p>
<h2 id="knn">KNN</h2>
<p>L‚Äôalgorithme des KNN consiste √† affecter √† un individu la classe dominante dans son voisinnage. En
supprimant les variables qualitatives (X46 √† X50), une LOOCV permet de montrer que le nombre de voisin
optimal est k = 15 avec un taux d‚Äôerreur de classification √† 0.572. Ce taux d‚Äôerreur √©lev√© peut s‚Äôexpliquer par
plusieurs facteurs:</p>
<ul>
<li>dans l‚Äôespace de d√©part les centres d‚Äôinertie des classes sont proches les unes des autres</li>
<li>la proportion in√©gale des classes pousse le classifieur √† toujours choisir la classe 2</li>
<li>la distance euclidienne utilis√©e par d√©faut est tr√®s sensible √† un nombre √©lev√© de pr√©dicteurs</li>
</ul>
<p>Les techniques d‚Äôanalyses discriminantes vues en cours peuvent √™tre facilement √©tendues √† des probl√®mes
de classification multi-classe car l‚Äôappartenance √† une classe revient √† trouver la probabilit√© √† posteriori
maximale pour l‚Äôindividu donn√©.</p>
<h2 id="analyse-discriminante">Analyse discriminante</h2>
<p>Puisque les fronti√®res construites par la LDA sont lin√©aires et que dans l‚Äôespace de d√©part, il n‚Äôy a pas
d‚Äôhyperplans discriminants apparents, la qualit√© du classifieur qui en r√©sulte est m√©diocre avec un taux
d‚Äôerreur de classification de 0.42.</p>
<p>De plus, un test de Barlett sur le jeu de donn√©es permet de rejetter l‚Äôhypoth√®se nulle d‚Äôhomosc√©dasticit√©
entre les diff√©rentes classes √† un niveau de confiance √† $2.2e^{‚àí16}$. Nous pouvons donc tenter d‚Äô√©liminer cette
hypoth√®se en utilisant l‚Äôanalyse discriminante quadratique (QDA). Avec une validation crois√©e √† 10 plis, nous
obtenons les taux d‚Äôerreur de classification suivant:</p>

<img src="/img/sy19_lda.png"  class="center"  />


<p>La variance √©lev√©e observ√©e d‚Äôun mod√®le QDA par rapport √† un mod√®le LDA peut s‚Äôexpliquer par le fait qu‚Äôil
doit estimer plus de param√®tres sur un √©chantillon de taille relativement faible. De plus, la QDA fait moins
d‚Äôerreur de classification pour les individus de classe 3 et l√©g√®rement plus d‚Äôerreur pour les individus de classe
1, ce qui donne en g√©n√©rale un meilleur mod√®le sachant que $œÄ_3 &gt; œÄ_1$.</p>
<p>Les mod√®les √©tudi√©es ci-dessus doivent estimer un nombre de param√®tres proportionnel aux nombres de
variables et de classes. Or, dans notre cas, avec 50 variables et 3 classes, la QDA doit estimer 3977 param√®tres
et la LDA 1427, ce qui va faire exploser la variance des mod√®les obtenus.</p>
<p>Pour palier √† ce probl√®me, en supposant l‚Äôind√©pendance des pr√©dicteurs conditionnellement √† leurs classes, on
peut construire un classifieur bay√©sien na√Øf avec moins de param√®tres √† estimer et donc plus adapt√© √† notre
nombre de variables. Malgr√© sa simplicit√©, le bay√©sien na√Øf offre une meilleure performance g√©n√©rale que la
LDA et un meilleure classification de la classe 1 que la QDA.</p>
<h3 id="r√©gularisation">R√©gularisation</h3>
<p>L‚Äôanalyse discriminante r√©gularis√©e (RDA) permet de trouver un compromis entre le biais √©lev√© des LDA et
la variance √©lev√©e des QDA. Au lieu d‚Äôestimer une matrice de variance commune (\hat{\Sigma}) comme dans la LDA ou
des matrices de variance ($\hat{\Sigma}_k$) pour chaque classe, elle construit une matrice de covariance r√©gularis√©e de la
forme: $\hat{\Sigma}_k(\lambda) = (1 ‚àí \lambda)\hat{\Sigma}_k + \lambda\hat{\Sigma}$. Lorsque $\lambda \rightarrow 0$, le classifieur se rapproche d‚Äôune QDA et lorsque $\lambda \rightarrow 1$, le
mod√®le tends vers une LDA.</p>

<img src="/img/sy19_rda.png"  class="center"  />


<p>En faisant une recherche syst√®matique dans l‚Äôespace des \lambda, on peut conclure qu‚Äôun coefficient de r√©gularisation
$\lambda \in [0.50, 0.55]$ donne le mod√®le optimal qui favorise l√©g√®rement l‚Äôhomosc√©dasticit√© des variances.
L‚Äôanalyse RDA admet aussi un coefficient $\gamma$ qui permet de r√©gulariser $\hat{\Sigma}$ selon la formule $(1 ‚àí \gamma)\hat{\Sigma} + \gamma\sigma^2
I_p$.</p>
<p>La fonction <code>rda</code> dans la librairie <code>klaR</code> qui permet d‚Äôoptimiser les deux param√®tres n‚Äôest pas stable pour les
jeu de donn√©es de petite taille. Nous pouvons lancer plusieurs it√©rations et identifier les zones optimales
o√π se trouvent le plus fr√©quemment les solutions et o√π le taux d‚Äôerreur de classification est le plus faible.
Nous trouvons donc que les couples de param√®tres optimales sont comprises entre $\lambda \in [0.04, 0.05]$ (ce qui
correspond √† une QDA) et $\gamma \in [0.50, 0.055]$.</p>

<img src="/img/sy19_qda.png"  class="center"  />


<h3 id="stepwise-selection">Stepwise selection</h3>
<p>La fonction <code>stepclass</code> de la librairie <code>klaR</code> adapte la stepwise selection √† la classification et donne directement
le sous-ensemble de pr√©dicteur optimal qui garantit le taux d‚Äôerreur de classification issu d‚Äôune validation
crois√©e le plus faible. Nous observons donc que cette s√©lection des variables √©change la performance moyenne
du classifieur pour r√©duire la variance du mod√®le</p>
<h2 id="r√©gression-logistique">R√©gression Logistique</h2>
<p>Contrairement √† l‚Äôanalyse discriminante lin√©aire, la r√©gression logistique est un mod√®le plus robuste qui ne
pr√©sume pas que les pr√©dicteurs suivent une distribution gaussienne multidimensionnelle et fonctionne avec
les variables qualitatives. Tout de m√™me, en appliquant une r√©gression multinomiale sur l‚Äôensemble du jeu de
donn√©es, le taux moyen d‚Äôerreur de classification issue d‚Äôune validation crois√©e √† 10-plis est aux alentours de
0.45 et d√©passe celui obtenu avec une LDA.</p>
<p>Pour am√©liorer cette performance, il serait judicieux d‚Äô√©liminer les pr√©dicteurs ‚Äúredondants‚Äù qui sont trop
fortement correl√©s entre eux puis les pr√©dicteurs √† effet n√©gligeable sur le mod√®le. La matrice de correlation
des donn√©es, elle, r√©v√®le aucune correlation forte entre les pr√©dicteurs donc il faut remettre en question la
significativit√© des variables. Les m√©thodes de r√©gularisation qui suppriment les coefficients n√©gligeables tel
que le lasso ou qui les p√©nalisent tel que l‚Äôelastic net peuvent r√©soudre ce probl√®me</p>

<img src="/img/sy19_logreg.png"  class="center"  />


<p>Avant d‚Äôappliquer la r√©gularisation, les variables quantitatives sont centr√©es et r√©duites afin d‚Äôaugmenter la
stabilit√© des estimations des coefficients et donc du mod√®le puis rattach√©es aux variables qualitatives. Nous
observons alors que la r√©gularisation r√©duit de mani√®re non n√©gligeable le taux d‚Äôerreur de classification de la
r√©gression logistique.</p>
<p>En prenant la valeur de lambda la plus r√©gularis√©e selon la r√®gle du 1-se, les coefficients $\beta$ obtenus entre le
lasso et l‚Äôelasticnet sont proches et r√©v√®lent l‚Äôimportance des variables qualitatives dans la classification des
individus 1 et 2.</p>

<img src="/img/sy19_lasso.png"  class="center"  />


<h2 id="arbres-et-for√™ts-al√©atoires">Arbres et for√™ts al√©atoires</h2>
<p>En r√©gularisant l‚Äôarbre de d√©cision par post-√©lagage, nous obtenons une erreur de validation crois√©e optimale
pour un arbre de profondeur 5 de 0.46. Avec la for√™t al√©atoire, nous obtenons un mod√®le de variance plus
faible mais fortement biais√© (la for√™t d√©cide rarement pour la classe 1) avec une erreur OOB de 0.394. M√™me
apr√®s avoir augment√© le poids de cette classe dans l‚Äôapprentissage, ce probl√®me n‚Äôest pas r√©solu.</p>
<h2 id="conclusion">Conclusion</h2>
<table>
<thead>
<tr>
<th align="center">Mod√®le</th>
<th align="center">Taux d‚Äôerreur de classification</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">KNN</td>
<td align="center">0.572</td>
</tr>
<tr>
<td align="center">LDA</td>
<td align="center">0.616</td>
</tr>
<tr>
<td align="center">LDA subset</td>
<td align="center">0.375</td>
</tr>
<tr>
<td align="center">QDA</td>
<td align="center">0.321</td>
</tr>
<tr>
<td align="center">QDA subset</td>
<td align="center">0.314</td>
</tr>
<tr>
<td align="center">RDA</td>
<td align="center">0.303</td>
</tr>
<tr>
<td align="center">RDA subset</td>
<td align="center">0.303</td>
</tr>
<tr>
<td align="center">Logit</td>
<td align="center">0.450</td>
</tr>
<tr>
<td align="center">Logit Ridge</td>
<td align="center">0.386</td>
</tr>
<tr>
<td align="center">Logit Elasticnet</td>
<td align="center">0.376</td>
</tr>
<tr>
<td align="center">Logit Lasso</td>
<td align="center">0.376</td>
</tr>
<tr>
<td align="center">Random Forest</td>
<td align="center">0.394</td>
</tr>
</tbody>
</table>
<p>Le mod√®le RDA est donc optimal pour classifier ce jeu de donn√©es. Dans nos futurs travaux, il serait
int√©ressant d‚Äôadresser le d√©s√©quilibre des classes (avec du under-sampling ou du SMOTE) et de tester d‚Äôautre
m√©thodes supervis√©es (SVM ou SGD).</p>
<hr>
<h1 id="r√©gression">R√©gression</h1>
<p>L‚Äôobjectif de ce travail est de proposer la meilleure r√©gression sur la variable y (E[y] = 161.65) √† partir d‚Äôun
√©chantillon de 500 r√©alisations de 100 variables. Similairement au probl√®me de classification, aucune une
signification peut √™tre attribuer √† chaque variable. Tout de m√™me, contrairement aux donn√©es de classification,
l‚Äôensemble des variables sont quantitatives continues d‚Äôesp√©rance communes proches de E[X] = 5 qui est
constitu√© de deux groupes:</p>
<ul>
<li>un groupe majoritaire suivant une distribution bimodale continue</li>
<li>une minorit√© (dont notamment X50, X65) qui suit peut-√™tre une distribution multimodale (k = 3)</li>
</ul>
<p>Quant aux r√©alisations de la variable √† pr√©dire, elle, suit clairement une loi normale avec $\hat{\mu} = 161.65$ et
$\hat{\sigma} = 82.67529$.</p>

<img src="/img/sy19_prelim_reg.png"  class="center"  />


<h2 id="feature-selection-1">Feature selection</h2>
<p>Pour ce jeu de donn√©es, le probl√®me du rapport entre le nombre de pr√©dicteurs et le nombre d‚Äôindivdus est
encore plus prononc√© (1/5). D‚Äôo√π la n√©cessit√© de r√©duire si possible la dimension par PCA. Tout de m√™me,
afin de maintenir 90 de la variance expliqu√©e cumul√©e, 80 axes doivent √™tre maintenus et la m√©thode du coude
ne r√©v√®le pas non plus un nombre de composantes principales optimal.</p>

<img src="/img/sy19_pca.png"  class="center"  />


<h2 id="knn-1">KNN</h2>
<p>L‚Äôalgorithme des knn se fonde sur un vote majoritaire des individus dans le nuage. Cette proximit√© se d√©finit
par une distance (euclidienne par d√©faut dans l‚Äôimpl√©mentation dans la librairie <code>MASS</code>) commun par rapport √†</p>
<p>tous les dimensions. Or, dans un jeu de donn√©es non standardis√©s, les √©chelles entre les dimensions peuvent
diff√©rer. D‚Äôo√π, la n√©cessit√© de normaliser (centrage et r√©duction) au pr√©alable les donn√©es.
Avec ce pr√©traitement sur l‚Äôensemble des pr√©dicteurs (y exclu), l‚Äôerreur quadratique moyenne du mod√®le avec
le nombre de voisins optimal obtenue par validation crois√©e imbriqu√©e am√©liore de mani√®re n√©gligeable de
5099.594 √† 5082.458. Malgr√© cette performance m√©diocre, ceci confirme l‚Äôobservation que les donn√©es initiales
sont relativement ‚Äúhomog√®nes‚Äù (moyenne et √©cart-type similaire).</p>
<h2 id="mod√®les-lin√©aires">Mod√®les lin√©aires</h2>
<p>De premi√®re vue, il est tr√®s probable que y soit une combinaison lin√©aire des diff√©rents pr√©dicteurs et donc
qu‚Äôun mod√®le lin√©aire sera optimal pour notre probl√©matique.</p>
<p>Dans un premier temps, nous avons estim√© le vecteur des coefficients $\hat{\beta}$ de la r√©gression par la m√©thode des
moindres carr√©s sur 75 des observations. Le mod√®le qui en r√©sulte est bien significatif car nous obtenons une
pvalue &lt; $2.2e^{‚àí16}$ pour l‚Äôhypoth√®se de nullit√© de l‚Äôensemble des param√®tres.</p>

<img src="/img/sy19_qqplot.png"  class="center"  />


<p>De plus, nous constatons que les r√©sidus ne semble pas suivre une structure particuli√®re et que les r√©sidus
standardis√©s √©pousent bien le qqplot d‚Äôune loi normale. Ceci confirme l‚Äôhypoth√®se de normalit√© des r√©sidus et
nous pouvons donc tenter de faire une validation crois√©e pour estimer la performance du LSE sur nos donn√©es.
Nous obtenons donc une bonne erreur quadratique moyenne de 260.1426 pour un mod√®le relativement simple.</p>
<h3 id="subset-selection">Subset selection</h3>
<p>A partir du mod√®le pr√©c√©dent, une question peut se poser: est-ce qu‚Äôil y a certaines variables non significatives
ou qui nuisent √† la r√©gression? En √©tudiant l‚Äô√©volution du coefficient de corr√©lation $R^2$ en fonction du nombre
de pr√©dicteurs pris en compte, la m√©thode du subset selection permet de montrer que pour maintenir un
mod√®le de performance similaire, il suffit de maintenir 80 variables.</p>

<img src="/img/sy19_mse.png"  class="center"  />


<p>L‚Äôusage de cette m√©thode permet de r√©duire la variance du mod√®le obtenu ainsi que la moyenne des erreurs
quadratiques observ√©es √† 235.1.</p>
<h3 id="r√©gularisation-1">R√©gularisation</h3>
<p>Similairement √† la r√©gression logistique dans la classification, il est possible de r√©gulariser la r√©gression lin√©aire
en p√©nalisant les coefficients par la m√©thode Ridge ou le Lasso. En faisant une validation crois√©e imbriqu√©e
pour optimiser les valeurs de $\lambda$, nous obtenons cette comparaison de la qualit√© des diff√©rentes m√©thodes de
r√©gularisation.</p>

<img src="/img/sy19_ridge.png"  class="center"  />


<p>Nous observons donc que parmi les r√©gularisations, le Ridge et le mod√®le de compromis (Elastic Net) d√©passent
clairement le lasso en terme de performance. Ceci sugg√®re que le nombre de variables important dans la
pr√©diction de y est important et la suppression de ces variables par lasso n‚Äôest pas n√©cessaire. De plus, nous
observons que la standardisation (centrage et r√©duction) des donn√©es activ√©e par d√©faut dans la fonction
glmnet affaiblit le lasso puisqu‚Äôelle acc√©l√®re peut-√™tre trop fortement la contraction des coefficients.</p>
<h3 id="splines-et-gam">Splines et GAM</h3>
<p>Il est possible que la variable √† pr√©dire r√©sulte d‚Äôune combinaison non lin√©aire des pr√©dicteurs. Comme nous
avons un jeu de donn√©es de dimension &gt; 2, deux mod√®les multidimensionnels ont √©t√© abord√©s en cours:</p>
<ul>
<li>Les splines multidimensionnels qui combinent les diff√©rentes fonctions de transformation par produit.
Avec 100 pr√©dicteurs, il est impossible d‚Äôappliquer ce mod√®le sans s√©lection pr√©alable √† cause du nombre
de param√®tres √† estimer.</li>
<li>Les mod√®les additifs g√©n√©ralis√©s (GAM) qui cherchent 100 fonctions transformant chacun une variable
au lieu d‚Äôune fonction √† 100 dimensions.</li>
</ul>
<p>En appliquant le gam avec un type de spline unique sur l‚Äôensemble des pr√©dicteurs sans standardisation, nous
obtenons les erreur quadratique suivante √† l‚Äôissu d‚Äôune validation crois√©e √† 10 plis:</p>

<img src="/img/sy19_gam.png"  class="center"  />


<p>La forme des b-splines cubiques (<code>gam bs(df=3)</code>) et des splines naturels (<code>gam ns(df=3)</code>) sont tr√®s similaires
mis √† part au niveau des fronti√®res de l‚Äôespace de y. Comme le gam est additif, ces similarit√©s s‚Äôaggr√®gent
dans le mod√®le final et explique la proximit√© entre les erreurs.</p>

<img src="/img/sy19_spline.png"  class="center"  />


<p>Quant aux smoothing splines avec un degr√© de libert√© de 2, nous obtenons un mod√®le de performance
relativement proche au subset. En examinant de pr√®s ce mod√®le, nous observons que seulement 68 variables
sont significatives (√† l‚Äôissu d‚Äôune analyse de variance param√©trique) et qu‚Äôil n‚Äôy aucune transformation de la
pr√©diction (fonction de lien: <code>identity</code>).</p>
<h2 id="conclusion-1">Conclusion</h2>
<table>
<thead>
<tr>
<th align="center">Mod√®le</th>
<th align="center">MSE moyenne</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">KNN</td>
<td align="center">5082.458</td>
</tr>
<tr>
<td align="center">Linear Regression (LR)</td>
<td align="center">260.1426</td>
</tr>
<tr>
<td align="center">LR subset selection</td>
<td align="center">235.1</td>
</tr>
<tr>
<td align="center">Ridge regression</td>
<td align="center">259.8</td>
</tr>
<tr>
<td align="center">Elasticnet regularisation</td>
<td align="center">249.9</td>
</tr>
<tr>
<td align="center">Lasso regularisation (without standardisation)</td>
<td align="center">317.5</td>
</tr>
<tr>
<td align="center">GAM (b-spline,df=3)</td>
<td align="center">429.3</td>
</tr>
<tr>
<td align="center">GAM (natural spline, df=2)</td>
<td align="center">294.8</td>
</tr>
<tr>
<td align="center">GAM (smoothing spline, df=2)</td>
<td align="center">233.2</td>
</tr>
</tbody>
</table>
<p>Le GAM constitu√© uniquement de smoothing spline sur l‚Äôensemble des pr√©dicteurs non standardis√©s est donc
optimal pour notre jeu de donn√©es car il donne l‚Äôerreur quadratique moyenne minimal ainsi qu‚Äôune variance
relativement faible.</p>
<p>Dans nos futurs travaux, il serait int√©ressant d‚Äôeffectuer une s√©lection pr√©alable des variables avant le
GAM et tester d‚Äôautre mod√®les comme les machines √† vecteurs de support (SVM) ou m√™me les splines
multidimensionnels (apr√®s une r√©duction de dimension par manifold learning).</p>

            </div>
        </article>
        <h2 id="end"></h2>
        <hr />

        <div class="post-info">
  		</div>
    </main>

            </div>

            
                <footer class="footer">
    <div class="footer__inner">
        <div class="footer__content">
            <span>&copy; 2022</span>
            
            <span><a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0</a></span>
        </div>
    </div>
    <div class="footer__inner">
        <div class="footer__content">
            <span><a href="https://themes.gohugo.io/hugo-theme-hello-friend-ng/">hello-friend-ng</a> Theme for <a href="http://gohugo.io">Hugo</a> by <a href="https://github.com/rhazdon">Djordje Atlialp</a></span>
        </div>
    </div>
</footer>

            
        </div>

        




<script type="text/javascript" src="/bundle.min.dc716e9092c9820b77f96da294d0120aeeb189b5bcea9752309ebea27fd53bbe6b13cffb2aca8ecf32525647ceb7001f76091de4199ac5a3caa432c070247f5b.js" integrity="sha512-3HFukJLJggt3&#43;W2ilNASCu6xibW86pdSMJ6&#43;on/VO75rE8/7KsqOzzJSVkfOtwAfdgkd5BmaxaPKpDLAcCR/Ww=="></script>



    </body>
</html>
